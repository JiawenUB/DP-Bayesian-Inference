% \documentclass[sigconf]{acmart}
\documentclass{article}

% We want page numbers on submissions

%%\ccsPaper{9999} % TODO: replace with your paper number once obtained
\input{macros}
\usepackage{accents}

\begin{document}
\title{Tailoring Differentially Private Bayesian Inference to Distance Between Distributions}

\author[*]{Jiawen Liu}
\author[**]{Mark Bun}
\author[*]{Gian Pietro Farina}
\author[*]{Marco Gaboardi}
\affil[*]{Department of Computer Science and Engineering, University at Buffalo, SUNY. \{jliu223,gianpiet,gaboardi\}@buffalo.edu}
\affil[**]{Department of Computer Science, Princeton University. {mbun@cs.princeton.edu}}
\date{}
\maketitle



\section{Preliminaries}
\label{sec_background}

\noindent \textbf{Bayesian Inference.}

Given a prior belief $\Pr(\theta)$ on some parameter $\theta$,
and an observation $\dataobs$, the posterior distribution on $\theta$ given $\dataobs$ is computed as:
\[
  \Pr(\theta | \dataobs) = \frac{\Pr(\dataobs | \theta) \cdot \Pr(\theta)}{\Pr(\dataobs)}
\]
where the expression $\Pr(\dataobs | \theta)$ denotes the
\emph{likelihood} of observing $\dataobs$ under a value of
$\theta$. Since we consider $\dataobs$ to be fixed, the likelihood is
a function of $\theta$.
For the same reason $\Pr(\dataobs)$ is a constant independent of $\theta$.
Usually in statistics the prior distribution $\Pr(\theta)$ is chosen so that it represents
the initial belief on $\theta$, that is, when no data has been observed. In practice though,
prior distributions and likelihood functions are usually chosen so that the posterior
belongs to the same \emph{family} of distributions. In this case we say that the prior
is conjugate to the likelihood function. Use of a conjugate prior
simplifies calculations and allows for inference to be performed in a
recursive fashion over the data.


\noindent \textbf{Beta-binomial System.}

In this work we will consider a specific instance of Bayesian inference and one of its generalizations.
specifically, a Beta-binomial mode. We will consider the situation the underlying data is binomial distribution ($\thicksim binomial(\theta)$), where $\theta$ represents
the parameter --informally called \emph{bias}-- of a Bernoulli
distributed random variable. The
prior distribution over $\theta\in [0,1]$ is going to be a beta
distribution, $\betad(\alpha, \beta)$, with parameters
$\alpha,\beta\in\mathbb{R}^{+}$, and with p.d.f:
\[
  \Pr(\theta)\equiv \frac{\theta^{\alpha} (1- \theta)^{\beta}}{\betaf(\alpha,\beta)}
\]
where $\betaf(\cdot,\cdot)$ is the beta function.
The data $\dataobs$ will be a sequence of $n\in\mathbb{N}$ binary values, that is $\dataobs= (x_1,\dots x_n), x_i\in\{0,1\}$, and the likelihood function is:
\[
  \Pr(\dataobs | \theta)\equiv \theta^{\Delta \alpha}(1-\theta)^{n - \Delta \alpha}
\]
where $\Delta \alpha = \displaystyle\sum_{i=1}^{n}x_i$.
From this it can easily be derived that the posterior distribution is:
\[
  \Pr(\theta|\dataobs)=\betad(\alpha + \Delta \alpha,\beta + n - \Delta \alpha)
\]


\noindent \textbf{Dirichlet-multinomial Systems.}

The beta-binomial model can be immediately generalized to Dirichlet-multinomial, with underlying data multinomially distributed. The \emph{bias} is represented by parameter $\vtheta$, the vector of parameters of a categorically distributed random variable. The prior distribution over $\vtheta\in [0,1]^{k}$
is given by a Dirichelet distribution, $\dirichlet(\valpha)$, for $k\in\mathbb{N}$,
and $\valpha\in(\mathbb{R}^{+})^{k}$, with p.d.f:
\[
  \Pr(\vtheta)\equiv\frac{1}{\mbetaf(\valpha)}\cdot \displaystyle\prod_{i=1}^{k}{\theta_i^{\alpha_i-1}}
\]
where $\mbetaf(\cdot)$ is the generalized beta function.
The data $\dataobs$ will be a sequence of $n\in\mathbb{N}$ values
coming from a universe $\datauni$, such that $\mid\datauni \mid=k$.
The likelihood function will be:
\[
  \Pr(\dataobs|\vtheta)\equiv \displaystyle\prod_{a_i\in\datauni}\theta_{i}^{\Delta \alpha_i},
\]
with $\Delta \alpha_i=\displaystyle\sum_{j=1}^{n}\iverson{x_j=a_i}$, where $\iverson{\cdot}$ represents Iverson bracket notation.
Denoting by $\Delta\valpha$ the vector $(\Delta\alpha_1,\dots \Delta\alpha_k)$ the posterior distribution over $\vtheta$ turns out to be
\[
  \Pr(\vtheta|\dataobs)=\dirichlet(\valpha+\Delta \valpha). 
\]
where $+$ denotes the componentwise sum of vectors of reals. 

\noindent \textbf{Differential Privacy.} 
\begin{definition}
\label{def_epsilon_dp}
$\epsilon-$differential privacy.

A randomized mechanism $\mathcal{M}: \mathcal{X} \rightarrow \mathcal{Y}$ is differential privacy, iff for any adjacent\footnote{Given $\dataobs, \dataobs'$  we say that $\dataobs$ and $\dataobs'$ are adjacent and we write, $\adj{\dataobs}{\dataobs'}$, iff\\
$\displaystyle \sum_{i}^{n}\iverson{x_i = x'_i }\leq 1$. } input $\dataobs, \dataobs' \in \mathcal{X}$, a metric $H$ over $\mathcal{Y}$ and a $B \subseteq H(\mathcal{Y})$, $\mathcal{M}$ satisfies:
\begin{equation*}
\mathbb{P}[H(\mathcal{M}(\dataobs)) \in B] = e^\epsilon \mathbb{P}[H(\mathcal{M}(\dataobs')) \in B].
\end{equation*}

\end{definition}

\begin{definition}
\label{def_epsilon_delta_dp}
$(\epsilon,\delta)-$differential privacy.

A randomized mechanism $\mathcal{M}: \mathcal{X} \rightarrow \mathcal{Y}$ is differential privacy, iff for any $\adj{\dataobs}{\dataobs'} \in \mathcal{X}$, a metric $H$ over $\mathcal{Y}$ and a $B \subseteq H(\mathcal{Y})$, $\mathcal{M}$ satisfies:
\begin{equation*}
\mathbb{P}[H(\mathcal{M}(\dataobs)) \in B] = e^\epsilon \mathbb{P}[H(\mathcal{M}(\dataobs')) \in B] + \delta,
\end{equation*}
vwhere $\dataobs = (x_i)_{i = 1}^n$ and $\dataobs = (x'_i)_{i = 1}^n$ is adjacent if there is only one $j$ that $x_j \neq x'_j$ and $x_i = x'_i$ for $i = 1, 2, \cdots, n; i \neq j$. 
\end{definition}



\section{$\gamma$-Smooth Sensitivity}
\label{sec_mechs}

\begin{definition}
Given data set $\dataobs$, $S(\dataobs)$ is the $\gamma -$smooth sensitivity of $\dataobs$, s.t. for any $\adj{\dataobs}{\dataobs'}$:

\begin{equation}
\label{eq_gamma_smooth}
\frac{1}{S(\dataobs)} - \frac{1}{S(\dataobs')} \leq \gamma.
\end{equation}
\end{definition}


\begin{thm}
\label{thm_gamma_smooth}
$S_\beta(\dataobs)$ is $\gamma -$smooth sensitivity for $\hellinger(\bysinfer(\dataobs),-)$ if it is calculated by:
\begin{equation}
  \label{eq_smooth}
   S(\dataobs)=\max_{\dataobs' \in \{0,1\}^{n}}\bigg \{ \frac{1}{\frac{1}{LS(\dataobs')} +\gamma \cdot d(\dataobs,\dataobs')} \bigg \},
\end{equation}
where $d$ is the Hamming distance between two data sets.
\end{thm}

\begin{proof}
of Theorem. \ref{thm_gamma_smooth}.
For $\adj{\dataobs}{\dataobs'}$ and arbitrary $\dataobs'' \in \{0,1\}^{n}$:

By Definition \ref{eq_smooth}:
\begin{equation*}
\begin{split}
S(\dataobs) 
& \geq \frac{1}{\frac{1}{LS(\dataobs'')} +\gamma \cdot d(\dataobs,\dataobs'')}\\
\frac{1}{S(\dataobs)} 
& \leq \frac{1}{LS(\dataobs'')} +\gamma \cdot d(\dataobs,\dataobs'')\\
\end{split}
\end{equation*}

Since $d(\dataobs,\dataobs'') \leq d(\dataobs,\dataobs') + d(\dataobs',\dataobs'') \leq 1 + d(\dataobs',\dataobs'')$:

\begin{equation*}
\begin{split}
& \leq \frac{1}{LS(\dataobs'')} +\gamma \cdot (1 + d(\dataobs',\dataobs''))\\
& = \gamma + \frac{1}{LS(\dataobs'')} +\gamma \cdot d(\dataobs',\dataobs'')\\
& = \gamma + \frac{1}{S(\dataobs')}
\end{split}
\end{equation*}


\end{proof}



\section{$\hexpmech$: Smoothed Hellinger Distance Based Exponential Mechanism}
\label{sec_mechs}
Given a prior distribution $\bprior = \betad(\alpha, \beta)$ and a sequence of $n$ observations $\dataobs\in\{0,1\}^n$, we define the follwing set:
\[
  \candidateset\dataobs \equiv\{\betad(\alpha',\beta')\mid \alpha'=\alpha+\Delta\alpha, \beta'=\beta+n-\Delta\alpha\},
\]
where $\Delta\alpha$ is as defined in Section\ref{sec_background}.
Notice that $\candidateset{\dataobs}$ has $n + 1$ elements, and
the Bayesian Inference process will produce an element from $\candidateset{\dataobs}$
that we denote by $\bysinfer(\dataobs)$ -- we don't explicitely
parametrize the result by the prior, which from now on we consider
fixed and we denote it by $\bprior$.

\begin{definition}
\label{def_smoo}
The mechanism $\hexpmech(\dataobs)$ outputs a candidate $r \in \candidateset\dataobs$ with probability
\begin{equation*}
\underset{z \thicksim \hexpmech}{\Pr}[z=r] = \frac {exp\big(\frac{-\epsilon\cdot\hellinger(\bysinfer(\dataobs),r)}{2\cdot S(\dataobs)}\big)}
{\nomalizer{\dataobs}}.
\end{equation*}
where $S_\beta(\dataobs)$ is the $\gamma -$smooth sensitivity of $\hellinger(\bysinfer(\dataobs),-)$, calculated by Theorem \ref{thm_gamma_smooth}.
\end{definition}

\section{Privacy Analysis}

\begin{lem}
\label{lem_hexpmech_privacy}
$\hexpmech$ is $2\epsilon$-differential privacy.
\end{lem}

\begin{proof} of Lemma \ref{lem_hexpmech_privacy}.
  By Definition \ref{def_epsilon_delta_dp}, to proof Lemma \ref{lem_hexpmech_privacy}, we need to prove:

  For any $\adj{\dataobs}{\dataobs'} \in \mathcal{X}$ and any beta distribution $r$:
  \begin{equation*}
  \hexpmechPr{\dataobs}{z = r} \leq e^{2\epsilon} \hexpmechPr{\dataobs'}{z = r}. 
  \end{equation*}

  By definition \ref{def_smoo}:

  \begin{equation*}
  \begin{split}
  \hexpmechPr{\dataobs}{z = r} 
  & = \frac {exp\big(\frac{-\epsilon\cdot\ux{r}}{2\cdot S(\dataobs)}\big)}{\unomalizer{\dataobs}} \\
  & = \frac {exp\big(
  \frac{-\epsilon\cdot(\ux{r} + \uxadj{r} - \uxadj{r})}{2\cdot S(\dataobs)}
  \big)}
  {\unomalizer{\dataobs}} \\
  & = \frac {exp\big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs)}
  \big)}
  {\unomalizer{\dataobs}}
  \cdot exp\big( \frac{\epsilon\cdot(\uxadj{r} - \ux{r})}{2\cdot S(\dataobs)} \big)\\
  \end{split}
  \end{equation*}

  Because $S(\dataobs) \geq LS(\dataobs) \geq (\uxadj{r} - \ux{r})$:
  \begin{equation*}
  \begin{split}
  & \leq \frac {exp\big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs)}
  \big)}
  {\unomalizer{\dataobs}}
  \cdot exp\big( \frac{\epsilon}{2} \big) \\
  & = exp\big( \frac{\epsilon}{2} \big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs)}
  \big)
  } 
  {\unomalizer{\dataobs}}
  exp
  \big(
  \frac{\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)\\
  & = exp\big( \frac{\epsilon}{2} \big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {\unomalizer{\dataobs}}
  exp
  \big(
  \frac{\epsilon\cdot(\uxadj{r})}{2}(\frac{1}{S(\dataobs')} - \frac{1}{S(\dataobs)})
  \big)\\
  \end{split}
  \end{equation*}
  
  Because $\uxadj{r} = \hellinger(\bysinfer(\dataobs'), r) \leq 1$:

  \begin{equation*}
  \begin{split}
  & \leq exp\big( \frac{\epsilon}{2} \big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {\unomalizer{\dataobs}}
  exp
  \big(
  \frac{\epsilon}{2}(\frac{1}{S(\dataobs')} - \frac{1}{S(\dataobs)})
  \big)\\
  \end{split}
  \end{equation*}

  Because the property of $\gamma -$ smooth sensitivity: $|\frac{1}{S(\dataobs')} - \frac{1}{S(\dataobs)}| \leq \gamma$:
  
  \begin{equation*}
  \begin{split}
  & \leq exp\big( \frac{\epsilon}{2} \big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {\unomalizer{\dataobs}}
  exp
  \big(
  \frac{\epsilon}{2} \cdot \gamma
  \big)\\
  & = exp\big( \frac{\epsilon}{2} + \frac{\epsilon}{2} \cdot \gamma\big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {\unomalizer{\dataobs}}\\
  \end{split}
  \end{equation*}
  Doing the same transformation in the denominator:

  \begin{equation*}
  \begin{split}
  & = exp\big( \frac{\epsilon}{2} + \frac{\epsilon}{2} \cdot \gamma\big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {
  \sum_{r'\in \candidateset{\dataobs}} 
  exp 
  \big(
  \frac{-\epsilon\cdot(\ux{r'} + \uxadj{r'} - \uxadj{r'})}{2 \cdot S(\dataobs)}
  \big)
  }\\
  & = exp\big( \frac{\epsilon}{2} + \frac{\epsilon}{2} \cdot \gamma\big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {
  \sum_{r'\in \candidateset{\dataobs}} 
  exp 
  \big(
  \frac{-\epsilon\cdot(\uxadj{r'}}{2 \cdot S(\dataobs)}
  \big)
  exp 
  \big(
  \frac{-\epsilon\cdot(\ux{r'} - \uxadj{r'})}{2 \cdot S(\dataobs)}
  \big)
  }\\
  \end{split}
  \end{equation*}

  Because $S(\dataobs) \geq LS(\dataobs) \geq (\ux{r} - \uxadj{r})$ $\implies$ $ \frac{-\epsilon\cdot(\ux{r'} - \uxadj{r'})}{2 \cdot S(\dataobs)} \geq \frac{-\epsilon}{2}$:

  \begin{equation*}
  \begin{split}
  & \leq exp\big( \frac{\epsilon}{2} + \frac{\epsilon}{2} \cdot \gamma\big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {
  \sum_{r'\in \candidateset{\dataobs}} 
  exp 
  \big(
  \frac{-\epsilon\cdot(\uxadj{r'}}{2 \cdot S(\dataobs)}
  \big)
  exp 
  \big(
  \frac{-\epsilon}{2}
  \big)
  }\\
  & = exp\big( \frac{\epsilon}{2} + \frac{\epsilon}{2} \cdot \gamma\big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {
  \sum_{r'\in \candidateset{\dataobs}} 
  exp 
  \big(
  \frac{-\epsilon\cdot(\uxadj{r'}}{2 \cdot S(\dataobs)}
  \big)
  exp 
  \big(
  \frac{-\epsilon}{2}
  \big)
  exp
  \big(
  \frac{\epsilon\cdot(\uxadj{r'})}{2\cdot S(\dataobs')}
  \big)
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r'})}{2\cdot S(\dataobs')}
  \big)
  }\\
  & = exp\big( \frac{\epsilon}{2} + \frac{\epsilon}{2} \cdot \gamma\big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {
  \sum_{r'\in \candidateset{\dataobs}} 
  exp 
  \big(
  \frac{-\epsilon\cdot(\uxadj{r'}}{2 \cdot S(\dataobs')}
  \big)
  exp 
  \big(
  \frac{-\epsilon}{2}
  \big)
  exp
  \big(
  \frac{- \epsilon\cdot(\ux{r'})}{2}
  (\frac{1}{S(\dataobs)}
-
  \frac{1}{S(\dataobs')})
  \big)
  }\\
  \end{split}
  \end{equation*}

  Because $\uxadj{r} = \hellinger(\bysinfer(\dataobs'), r) \leq 1$ $\implies$ $\frac{- \epsilon\cdot(\ux{r'})}{2} \geq  \frac{-\epsilon}{2}$:

    \begin{equation*}
  \begin{split}
  & \leq exp\big( \frac{\epsilon}{2} + \frac{\epsilon}{2} \cdot \gamma\big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {
  \sum_{r'\in \candidateset{\dataobs}} 
  exp 
  \big(
  \frac{-\epsilon\cdot(\uxadj{r'}}{2 \cdot S(\dataobs')}
  \big)
  exp 
  \big(
  \frac{-\epsilon}{2}
  \big)
  exp
  \big(
  \frac{- \epsilon}{2}
  (\frac{1}{S(\dataobs)}
-
  \frac{1}{S(\dataobs')})
  \big)
  }\\
  \end{split}
  \end{equation*}

  Because the property of $\gamma -$ smooth sensitivity: $|\frac{1}{S(\dataobs)} - \frac{1}{S(\dataobs')}| \leq \gamma$ $\implies$
  $\frac{- \epsilon}{2}
  (\frac{1}{S(\dataobs)}-
  \frac{1}{S(\dataobs')}) \geq \frac{- \epsilon}{2} \cdot \gamma$
      \begin{equation*}
  \begin{split}
  & \leq exp\big( \frac{\epsilon}{2} + \frac{\epsilon}{2} \cdot \gamma\big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {
  \sum_{r'\in \candidateset{\dataobs}} 
  exp 
  \big(
  \frac{-\epsilon\cdot(\uxadj{r'}}{2 \cdot S(\dataobs')}
  \big)
  exp 
  \big(
  \frac{-\epsilon}{2}
  \big)
  exp
  \big(
  \frac{- \epsilon}{2} \cdot \gamma
  \big)
  }\\
  & = exp\big( \frac{\epsilon}{2} + \frac{\epsilon}{2} \cdot \gamma\big) \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {
  \sum_{r'\in \candidateset{\dataobs}} 
  exp 
  \big(
  \frac{-\epsilon\cdot(\uxadj{r'}}{2 \cdot S(\dataobs')}
  \big)
  exp 
  \big(
  \frac{-\epsilon}{2} +   \frac{- \epsilon}{2} \cdot \gamma
  \big)
  }\\
  & = e^{(\epsilon + \epsilon \cdot \gamma )} \cdot 
  \frac {
  exp
  \big(
  \frac{-\epsilon\cdot(\uxadj{r})}{2\cdot S(\dataobs')}
  \big)
  } 
  {
  \sum_{r'\in \candidateset{\dataobs}} 
  exp 
  \big(
  \frac{-\epsilon\cdot(\uxadj{r'}}{2 \cdot S(\dataobs')}
  \big)
  }\\
  & = e^{( \epsilon + \epsilon \cdot \gamma )} \cdot   \hexpmechPr{\dataobs'}{z = r}
  \end{split}
  \end{equation*}

  By setting the $\gamma = 1$, $2\epsilon - $differential privacy can be achieved.
\end{proof}

\bibliographystyle{plain}
\bibliography{bayesian.bib}

\end{document}

