\documentclass{article}

\input{macros}
\usepackage{accents}

\begin{document}
\title{Optimality of Laplace Mechanism}

\author{Jiawen \textsc{Liu}} % Author name

\maketitle

\section{Laplace Mechanism with Post-Processing}

\begin{algorithm}
  \caption{$\lapmech$}
  \label{lapmech}
  \begin{algorithmic}
  	\REQUIRE $\dataobs \in \{0 ,1 \}^n$
	\STATE \quad apply the Bayesian inference algorithm on $\dataobs'$, get true posterior $\betad(\valpha)$
  	\STATE \quad {\bf let} $p = \uniform{0}{1}$, $\eta \sim \lap{0}{\frac{1.0}{\epsilon}}$
  	\STATE \quad {\bf If} $p >0.5$: 
  	\STATE \quad \quad with 0.5 probability adding noise to first component.
  	\STATE \quad \quad  $\dataobs' = (\lfloor \text{x}_1 + \mu \rfloor_0^n, n - \lfloor \text{x}_1 + \mu \rfloor_0^n)$ 
  	\STATE \quad {\bf Else}: 
  	\STATE \quad \quad with 0.5 probability adding noise to second component.
  	\STATE \quad \quad  $\dataobs' = (n - \lfloor \text{x}_2 + \mu \rfloor_0^n, \lfloor \text{x}_2 + \mu \rfloor_0^n)$ 
	\STATE \quad apply the Bayesian inference algorithm on $\dataobs'$, get: $\betad(\valpha')$
	\RETURN $\valpha'$
  \end{algorithmic}
\end{algorithm}



\newpage
\section{Laplace Mechanism Accuracy Analysis}
\label{sec_fullacc}



\begin{table}[htbp]
	\centering
	\caption{Accuracy with $n = 2, \epsilon = 1.0$}
	\label{tab_n2prob}
\begin{tabular}{|c||r|r|r|}
	\hline

	\multirow{2}{*}{True data ($\dataobs$)} 	
								& \multicolumn{3}{c|}{Noised Results ($\dataobs'$)}  		
								\\ \cline{2-4}
	                      		&  $\Pr[\dataobs' = (0, 2)]$  	
	                      		&  $\Pr[\dataobs' = (1, 1)]$ 	
	                      		&  $\Pr[\dataobs' = (2, 0)]$ 	\\  \hline \hline
	$\dataobs = (0,2)$          & {\bf 0.658030139707}	
								& {\bf 0.216166179191}	
								& 0.125803681102 
								\\  \hline
	$\dataobs = (1,1)$          & {\bf 0.341969860293}	
								& {\bf 0.316060279414} 			
								& 0.341969860293
								\\  \hline
	$\dataobs = (2,0)$          & 0.125803681102
								& {\bf 0.216166179191} 			
								& {\bf 0.658030139707}
								\\  \hline
\end{tabular}
\end{table}


The computation formula for each of the value in the Tbable \ref{tab_n2prob}:

{\small
\noindent $\dataobs = (0,2):$
\[
\Pr[\dataobs']
= \begin{cases}
\begin{array}{ll}
 	& 0.5 \times 
 	\big(
 	\Pr[	0	< \lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}			<	0] 
 	\big) \\
 	+ 	
 	& 0.5 \times 
 	\big(
 	\Pr[	0	< \lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 
 	\Pr[	1 	< \lap{0}{\frac{1}{\epsilon}}] 
 	\big) 
\end{array} 
	& \dataobs' = (0, 2)\\
\begin{array}{ll}
 	0.5 \times 
 	\big(
 	\Pr[	1	< \lap{0}{\frac{1}{\epsilon}}	<	2] 
 	\big)
 	+ 	
 	0.5 \times 
 	\big(
 	\Pr[	-1	< \lap{0}{\frac{1}{\epsilon}}	<	0] 
 	\big) 
\end{array}  
	& \dataobs' = (1, 1)\\
\begin{array}{ll}
 	& 0.5 \times 
 	\big(
 	\Pr[	2	<	\lap{0}{\frac{1}{\epsilon}}	<	3] 
 	+ 
 	\Pr[	3	<	\lap{0}{\frac{1}{\epsilon}}]
 	\big) \\
 	+ 	
 	& 0.5 \times 
 	\big(
 	\Pr[	-2	< \lap{0}{\frac{1}{\epsilon}}	<	-1] 
 	+ 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}			<	-2] 
 	\big) 
\end{array}  
& \dataobs' = (2, 0)
\end{cases}.
\]


\noindent $\dataobs = (1,1):$
\[
\Pr[\dataobs']
= \begin{cases}
\begin{array}{ll}
 	& 0.5 \times 
 	\big(
 	\Pr[	-1	< \lap{0}{\frac{1}{\epsilon}}	<	0] 
 	+ 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}			<	-1] 
 	\big) \\
 	+ 	
 	& 0.5 \times 
 	\big(
 	\Pr[	1	< \lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 
 	\Pr[	2 	< \lap{0}{\frac{1}{\epsilon}}] 
 	\big) 
\end{array} 
	& \dataobs' = (0, 2)\\
\begin{array}{ll}
 	0.5 \times 
 	\big(
 	\Pr[	0	< \lap{0}{\frac{1}{\epsilon}}	<	1] 
 	\big)
 	+ 	
 	0.5 \times 
 	\big(
 	\Pr[	0	< \lap{0}{\frac{1}{\epsilon}}	<	1] 
 	\big) 
\end{array}  
	& \dataobs' = (1, 1)\\
\begin{array}{ll}
 	& 0.5 \times 
 	\big(
 	\Pr[	1	<	\lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 
 	\Pr[	2	<	\lap{0}{\frac{1}{\epsilon}}]
 	\big) \\
 	+ 	
 	& 0.5 \times 
 	\big(
 	\Pr[	-1	< \lap{0}{\frac{1}{\epsilon}}	<	0] 
 	+ 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}			<	-1] 
 	\big) 
\end{array}  
& \dataobs' = (2, 0)
\end{cases}.
\]

\noindent $\dataobs = (2,0):$
\[
\Pr[\dataobs']
= \begin{cases}
\begin{array}{ll}
 	& 0.5 \times 
 	\big(
 	\Pr[	-2	< \lap{0}{\frac{1}{\epsilon}}	<	-1] 
 	+ 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}			< 	-2] 
 	\big) \\
 	+ 	
 	& 0.5 \times 
 	\big(
 	\Pr[	2	< \lap{0}{\frac{1}{\epsilon}}	<	3] 
 	+ 
 	\Pr[	3	< \lap{0}{\frac{1}{\epsilon}}] 
 	\big) 
\end{array} 
	& \dataobs' = (0, 2)\\
\begin{array}{ll}
 	0.5 \times 
 	\big(
 	\Pr[	-1	< \lap{0}{\frac{1}{\epsilon}}	<	0] 
 	\big)
 	+ 	
 	0.5 \times 
 	\big(
 	\Pr[	1	< \lap{0}{\frac{1}{\epsilon}}	<	2] 
 	\big) 
\end{array}  
	& \dataobs' = (1, 1)\\
\begin{array}{ll}
 	& 0.5 \times 
 	\big(
 	\Pr[	0	<	\lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 
 	\Pr[	1	<	\lap{0}{\frac{1}{\epsilon}}]
 	\big) \\
 	+ 	
 	& 0.5 \times 
 	\big(
 	\Pr[	0	< \lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}			<	0] 
 	\big) 
\end{array}  
& \dataobs' = (2, 0)
\end{cases}.
\]
}


\begin{table}[htbp]
	\centering
	\caption{Error (Hellinger Distance) Between Noised Posterior and True Posterior with $n = 2, \epsilon = 1.0$, prior: $\betad(1,1)$}
	\label{tab_n2error}
\begin{tabular}{|c||r|r|r|}
	\hline

	\multirow{2}{*}{True data ($\dataobs$)} 	
								& \multicolumn{3}{c|}{$\hellinger(\betad(\valpha), \betad(\valpha'))$}  		
								\\ \cline{2-4}
	                      		& $\dataobs' = (0, 2)$
	                      		& $\dataobs' = (1, 1)$
	                      		& $\dataobs' = (2, 0)$
	                      		\\  \hline \hline
	$\dataobs = (0,2)$          & 0.0	
								& 0.351365939808
								& 0.627271345023 
								\\  \hline
	$\dataobs = (1,1)$          & 0.351365939808	
								& 0.0			
								& 0.351365939808
								\\  \hline
	$\dataobs = (2,0)$          & 0.627271345023
								& 0.351365939808 			
								& 0.0
								\\  \hline
\end{tabular}
\end{table}


\newpage
$n = 3:$
\begin{table}[htbp]
	\vspace{-0.5cm}

	\footnotesize
	\centering
	\caption{Accuracy with $n = 3, \epsilon = 1.0$}
	\label{tab_n3prob}
\begin{tabular}{|c||r|r|r|r|}
	\hline

	\multirow{2}{*}{True data ($\dataobs$)}
								& \multicolumn{4}{c|}{Noised Results ($\dataobs'$)}  
								\\ \cline{2-5}
	                      		&  $\Pr[\dataobs' = (0, 3)]$  	
	                      		&  $\Pr[\dataobs' = (1, 2)]$ 	
	                      		&  $\Pr[\dataobs' = (2, 1)]$ 	
	                      		&  $\Pr[\dataobs' = (3, 0)]$ 	
	                      		\\  \hline
	                      		\hline
	$\dataobs = (0,3)$          & {\bf 0.658030139707}	
								& {\bf 0.216166179191}	
								&  0.0795230932009
								&  0.0462805879011
								\\  \hline
	$\dataobs = (1,2)$          & {\bf 0.341969860293}	
								& {\bf 0.316060279414}			
								& {\bf 0.216166179191}
								&  0.125803681102
								\\  \hline
	$\dataobs = (2,1)$          & 0.125803681102
								& {\bf 0.216166179191}			
								& {\bf 0.316060279414}
								& {\bf 0.341969860293} 
								\\  \hline
	$\dataobs = (3,0)$          & 0.0462805879011
								& 0.0795230932009 			
								& {\bf 0.216166179191}
								& {\bf 0.658030139707} 
								\\  \hline
\end{tabular}
\end{table}


The computation formula for each of the value in the table:

{\small

\noindent $\dataobs = (0,3):$
\[
\Pr[\dataobs']
= \begin{cases}
	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 	
 	0.5 \times 
 	\Pr[	0	< \lap{0}{\frac{1}{\epsilon}}] 
%
	& \dataobs' = (0, 3)\\
%
 	0.5 \times 
 	\Pr[	1	< \lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 	
 	0.5 \times 
 	\Pr[	-1	< \lap{0}{\frac{1}{\epsilon}}	<	0] 
%
	& \dataobs' = (1, 2)\\
%
 	0.5 \times 
 	\Pr[	2	<	\lap{0}{\frac{1}{\epsilon}}	<	3] 
 	+ 	
 	0.5 \times 
 	\Pr[	-2	< \lap{0}{\frac{1}{\epsilon}}	<	-1] 
%
	& \dataobs' = (2, 1)\\
%
 	0.5 \times 
 	\Pr[	3	<	\lap{0}{\frac{1}{\epsilon}}]
 	+ 	
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	-2] 
%
& \dataobs' = (3, 0)
\end{cases}.
\]


\noindent $\dataobs = (1,2):$
\[
\Pr[\dataobs']
= \begin{cases}
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	0] 
 	+ 	
 	0.5 \times 
 	\Pr[	1	< \lap{0}{\frac{1}{\epsilon}}] 
	& \dataobs' = (0, 3)\\
%
 	0.5 \times 
 	\Pr[	0	< \lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 	
 	0.5 \times 
 	\Pr[	0	< \lap{0}{\frac{1}{\epsilon}}	<	1] 
	& \dataobs' = (1, 2)\\
%
 	0.5 \times 
 	\Pr[	1	<	\lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 	
 	0.5 \times 
 	\Pr[	-1	< \lap{0}{\frac{1}{\epsilon}}	<	0] 
	& \dataobs' = (2, 1)\\
%
 	0.5 \times 
 	\Pr[	2	<	\lap{0}{\frac{1}{\epsilon}}] 
 	+ 	
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	-1] 
	& \dataobs' = (3, 0)
\end{cases}.
\]

\noindent $\dataobs = (2,1):$
\[
\Pr[\dataobs']
= \begin{cases}
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	-1]
 	+ 	
 	0.5 \times 
 	\Pr[	2	< \lap{0}{\frac{1}{\epsilon}}] 
%
	& \dataobs' = (0, 3)\\
%
 	0.5 \times 
 	\Pr[	-1	< \lap{0}{\frac{1}{\epsilon}}	<	0] 
 	+ 	
 	0.5 \times 
 	\Pr[	1	< \lap{0}{\frac{1}{\epsilon}}	<	2] 
%
	& \dataobs' = (1, 2)\\
%
 	0.5 \times 
 	\Pr[	0	<	\lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 	
 	0.5 \times 
 	\Pr[	0	< \lap{0}{\frac{1}{\epsilon}}	<	1] 
%
& \dataobs' = (2, 1)\\
%
 	0.5 \times 
 	\Pr[	1	<	\lap{0}{\frac{1}{\epsilon}}	] 
 	+ 	
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	0] 
%
& \dataobs' = (3, 0)
\end{cases}.
\]



\noindent $\dataobs = (3,0):$
\[
\Pr[\dataobs']
= \begin{cases}
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	-2] 
 	+ 	
 	0.5 \times 
 	\Pr[	3	< \lap{0}{\frac{1}{\epsilon}}] 
	& \dataobs' = (0, 3)\\
%
 	0.5 \times 
 	\Pr[	-2	< \lap{0}{\frac{1}{\epsilon}}	<	-1] 
 	+ 	
 	0.5 \times 
 	\Pr[	2	< \lap{0}{\frac{1}{\epsilon}}	<	3] 
	& \dataobs' = (1, 2)\\
%
 	0.5 \times 
 	\Pr[	-1	<	\lap{0}{\frac{1}{\epsilon}}	<	0] 
 	+ 	
 	0.5 \times 
 	\Pr[	1	< \lap{0}{\frac{1}{\epsilon}}	<	2] 
	& \dataobs' = (2, 1)\\
%
 	0.5 \times 
 	\Pr[	0	<	\lap{0}{\frac{1}{\epsilon}}	] 
 	+ 	
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	1] 
	& \dataobs' = (3, 0)
\end{cases}.
\]
}

\begin{table}[htbp]
	\vspace{-0.5cm}
	\centering
	\small
	\caption{Error (Hellinger Distance) with $n = 3, \epsilon = 1.0$, prior: $\betad(1,1)$}
	\label{tab_n3error}
\begin{tabular}{|c||r|r|r|r|}
	\hline

	\multirow{2}{*}{True data ($\dataobs$)}
								& \multicolumn{4}{c|}
								{$\hellinger(\betad(\valpha), \betad(\valpha'))$}  
								\\ \cline{2-5}
	                      		&  $\dataobs' = (0, 3)$  	
	                      		&  $\dataobs' = (1, 2)$ 	
	                      		&  $\dataobs' = (2, 1)$ 	
	                      		&  $\dataobs' = (3, 0)$ 	
	                      		\\  \hline
	                      		\hline
	$\dataobs = (0,3)$          & 0.0	
								& 0.309682680332	
								& 0.537732813886
								& 0.72945673866
								\\  \hline
	$\dataobs = (1,2)$          & 0.309682680332
								& 0.0
								& 0.272078233285
								& 0.537732813886
								\\  \hline
	$\dataobs = (2,1)$          & 0.537732813886
								& 0.272078233285			
								& 0.0
								& 0.309682680332
								\\  \hline
	$\dataobs = (3,0)$          & 0.72945673866
								& 0.537732813886 			
								& 0.309682680332
								& 0.0
								\\  \hline
\end{tabular}
\end{table}

\newpage
$n = 4:$
\begin{table}[htbp]
	\vspace{-0.5cm}
	\scriptsize
	\centering
	\caption{Accuracy with $n = 4, \epsilon = 1.0$}
	\label{tab_n4prob}
\begin{tabular}{|c||r|r|r|r|r|}
	\hline

	\multirow{2}{*}{True data ($\dataobs$)}
								& \multicolumn{5}{c|}{Noised Results ($\dataobs'$)}  
								\\ \cline{2-6}
	                      		&  $\Pr[\dataobs' = (0, 4)]$  	
	                      		&  $\Pr[\dataobs' = (1, 3)]$ 	
	                      		&  $\Pr[\dataobs' = (2, 2)]$ 	
	                      		&  $\Pr[\dataobs' = (3, 1)]$ 	
	                      		&  $\Pr[\dataobs' = (4, 0)]$ 	
	                      		\\  \hline
	                      		\hline
	$\dataobs = (0,4)$          & {\bf 0.658030139707}	
								& {\bf 0.216166179191}	
								&  0.0795230932009
								&  0.029254911087
								&  0.0170256768141
								\\  \hline
	$\dataobs = (1,3)$          & {\bf 0.341969860293}	
								& {\bf 0.316060279414}			
								& {\bf 0.216166179191}
								&  0.0795230932009
								&  0.0462805879011
								\\  \hline
	$\dataobs = (2,2)$          & 0.125803681102
								& {\bf 0.216166179191}			
								& {\bf 0.316060279414}
								& {\bf 0.216166179191} 
								&  0.125803681102
								\\  \hline
	$\dataobs = (3,1)$          & 0.0462805879011
								& 0.0795230932009 			
								& {\bf 0.216166179191}
								& {\bf 0.316060279414} 
								&  0.341969860293
								\\  \hline
	$\dataobs = (4,0)$          & 0.0170256768141
								& 0.029254911087 			
								& {\bf 0.0795230932009}
								& {\bf 0.216166179191} 
								&  0.658030139707
								\\  \hline
\end{tabular}
\vspace{-0.5cm}
\end{table}
{\scriptsize

\[
\dataobs = (0,4): \Pr[\dataobs']
= \begin{cases}
	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 	
 	0.5 \times 
 	\Pr[	0	\leq \lap{0}{\frac{1}{\epsilon}}] 
%
	& \dataobs' = (0, 4)\\
%
 	0.5 \times 
 	\Pr[	1	\leq \lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 	
 	0.5 \times 
 	\Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}	<	0] 
%
	& \dataobs' = (1, 3)\\
%
 	0.5 \times 
 	\Pr[	2	\leq	\lap{0}{\frac{1}{\epsilon}}	<	3] 
 	+ 	
 	0.5 \times 
 	\Pr[	-2	\leq \lap{0}{\frac{1}{\epsilon}}	<	-1] 
%
	& \dataobs' = (2, 2)\\
%
 	0.5 \times 
 	\Pr[	3	\leq	\lap{0}{\frac{1}{\epsilon}}	< 4]
 	+ 	
 	0.5 \times 
 	\Pr[	-3	\leq	\lap{0}{\frac{1}{\epsilon}}	<	-2] 
%
	& \dataobs' = (3, 1)\\
%
 	0.5 \times 
 	\Pr[	4	\leq	\lap{0}{\frac{1}{\epsilon}}]
 	+ 	
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	-3] 
%
	& \dataobs' = (4, 0)
\end{cases}.
\]

\[
(\dataobs = (1,3)) \Pr[\dataobs']
= \begin{cases}
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	0] 
 	+ 	
 	0.5 \times 
 	\Pr[	1	\leq \lap{0}{\frac{1}{\epsilon}}] 
	& \dataobs' = (0, 4)\\
%
 	0.5 \times 
 	\Pr[	0	\leq \lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 	
 	0.5 \times 
 	\Pr[	0	\leq \lap{0}{\frac{1}{\epsilon}}	<	1] 
	& \dataobs' = (1, 3)\\
%
 	0.5 \times 
 	\Pr[	1	\leq	\lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 	
 	0.5 \times 
 	\Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}	<	0] 
	& \dataobs' = (2, 2)\\
%
 	0.5 \times 
 	\Pr[	2	\leq	\lap{0}{\frac{1}{\epsilon}}	<	3] 
 	+ 	
 	0.5 \times 
 	\Pr[	-2	\leq	\lap{0}{\frac{1}{\epsilon}}	<	-1] 
	& \dataobs' = (3, 1)\\
 	0.5 \times 
 	\Pr[	3	\leq	\lap{0}{\frac{1}{\epsilon}}] 
 	+ 	
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	-2] 
	& \dataobs' = (4, 0)
\end{cases}.
\]



\[
(\dataobs = (2,2)) \Pr[\dataobs']
= \begin{cases}
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	-1]
 	+ 	
 	0.5 \times 
 	\Pr[	2	\leq \lap{0}{\frac{1}{\epsilon}}] 
%
	& \dataobs' = (0, 4)\\
%
 	0.5 \times 
 	\Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}	<	0] 
 	+ 	
 	0.5 \times 
 	\Pr[	1	\leq \lap{0}{\frac{1}{\epsilon}}	<	2] 
%
	& \dataobs' = (1, 3)\\
%
 	0.5 \times 
 	\Pr[	0	\leq	\lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 	
 	0.5 \times 
 	\Pr[	0	\leq \lap{0}{\frac{1}{\epsilon}}	<	1] 
%
& \dataobs' = (2, 2)\\
%
 	0.5 \times 
 	\Pr[	1	\leq	\lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 	
 	0.5 \times 
 	\Pr[	-1 	\leq	\lap{0}{\frac{1}{\epsilon}}	<	0] 
%
& \dataobs' = (3, 1)\\
%
 	0.5 \times 
 	\Pr[	2	\leq	\lap{0}{\frac{1}{\epsilon}}	] 
 	+ 	
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	-1] 
%
& \dataobs' = (4, 0)
\end{cases}.
\]


\[
( \dataobs = (3,1) ) \Pr[\dataobs']
= \begin{cases}
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	-2] 
 	+ 	
 	0.5 \times 
 	\Pr[	3	\leq \lap{0}{\frac{1}{\epsilon}}] 
	& \dataobs' = (0, 4)\\
%
 	0.5 \times 
 	\Pr[	-2	\leq \lap{0}{\frac{1}{\epsilon}}	<	-1] 
 	+ 	
 	0.5 \times 
 	\Pr[	2	\leq \lap{0}{\frac{1}{\epsilon}}	<	3] 
	& \dataobs' = (1, 3)\\
%
 	0.5 \times 
 	\Pr[	-1	<	\lap{0}{\frac{1}{\epsilon}}		<	0] 
 	+ 	
 	0.5 \times 
 	\Pr[	1	\leq \lap{0}{\frac{1}{\epsilon}}	<	2] 
	& \dataobs' = (2, 2)\\
%
 	0.5 \times 
 	\Pr[	0	\leq	\lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 	
 	0.5 \times 
 	\Pr[	0	\leq	\lap{0}{\frac{1}{\epsilon}}	<	1] 
	& \dataobs' = (3, 1)\\
%
 	0.5 \times 
 	\Pr[	1	\leq	\lap{0}{\frac{1}{\epsilon}}	] 
 	+ 	
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	0] 
%
& \dataobs' = (4, 0)
\end{cases}.
\]


\[
(\dataobs = (4,0)) \Pr[\dataobs']
= \begin{cases}
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	-3] 
 	+ 	
 	0.5 \times 
 	\Pr[	4	\leq \lap{0}{\frac{1}{\epsilon}}] 
	& \dataobs' = (0, 3)\\
%
 	0.5 \times 
 	\Pr[	-3	\leq \lap{0}{\frac{1}{\epsilon}}	<	-2] 
 	+ 	
 	0.5 \times 
 	\Pr[	3	\leq \lap{0}{\frac{1}{\epsilon}}	<	4] 
	& \dataobs' = (1, 3)\\
%
 	0.5 \times 
 	\Pr[	-2	\leq	\lap{0}{\frac{1}{\epsilon}}	<	-1] 
 	+ 	
 	0.5 \times 
 	\Pr[	2	\leq 	\lap{0}{\frac{1}{\epsilon}}	<	3] 
	& \dataobs' = (2, 2)\\
%
 	0.5 \times 
 	\Pr[	-1	\leq	\lap{0}{\frac{1}{\epsilon}}	<	0] 
 	+ 	
 	0.5 \times 
 	\Pr[	1	\leq	\lap{0}{\frac{1}{\epsilon}}	<	2] 
	& \dataobs' = (3, 1)\\
%
 	0.5 \times 
 	\Pr[	0	\leq	\lap{0}{\frac{1}{\epsilon}}	] 
 	+ 	
 	0.5 \times 
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	1] 
%
& \dataobs' = (4, 0)
\end{cases}.
\]
}

\begin{table}[htbp]
	\vspace{-0.5cm}
	\scriptsize
	\centering
	\caption{Error (Hellinger Distance) with $n = 4, \epsilon = 1.0$, prior: $\betad(1,1)$}
	\label{tab_n4error}
\begin{tabular}{|c||r|r|r|r|r|}
	\hline

	\multirow{2}{*}{True data ($\dataobs$)}
								& \multicolumn{5}{c|}
								{$\hellinger(\betad(\valpha), \betad(\valpha'))$}  
								\\ \cline{2-6}
								&  $\dataobs' = (0, 4)$  	
								&  $\dataobs' = (1, 3)$ 	
								&  $\dataobs' = (2, 2)$ 	
								&  $\dataobs' = (3, 1)$ 	
								&  $\dataobs' = (4, 0)$ 	
								\\  \hline
	 							\hline
	$\dataobs = (0,4)$			&	0.0	
								&	0.285148545618
								&	0.489330216753
								&	0.65381277487
								&	0.795060097621
								\\  \hline
	$\dataobs = (1,3)$          &	0.285148545618
								&	0.0
								&	0.236768870388
								&	0.45001855566
								&	0.65381277487
								\\  \hline
	$\dataobs = (2,2)$          &	0.489330216753
								&	0.236768870388			
								&	0.0
								&	0.236768870388
								&	0.489330216753
								\\  \hline
	$\dataobs = (3,1)$          &	0.65381277487
								&	0.45001855566			
								&	0.236768870388
								&	0.0
								&	0.285148545618
								\\  \hline
	$\dataobs = (4,0)$          &	0.795060097621
								&	0.65381277487 			
								&	0.489330216753
								&	0.285148545618
								&	0.0
								\\  \hline
\end{tabular}
\vspace{-0.5cm}
\end{table}


\newpage
{\small

\noindent $n = 2:$
\[
\Pr[\hlg(\bysinfer{\dataobs}, \ilapmech(\dataobs)) < 0.35137 ~ = LS(x)]
\geq \begin{cases}
	0.87	& \dataobs = (0, 2)\\
%
	1.0		& \dataobs = (1, 1)\\
%
	0.87	& \dataobs = (2, 0)\\
\end{cases}.
\]


\noindent $n = 3:$
\[
\Pr[\hlg(\bysinfer{\dataobs}, \ilapmech(\dataobs)) < 0.30968 ~ = LS(x)]
\geq \begin{cases}
 	0.87
	& \dataobs = (0, 3)\\
%
 	0.87
	& \dataobs = (1, 2)\\
%
 	0.87
	& \dataobs = (2, 1)\\
%
	0.87
	& \dataobs = (3, 0)
\end{cases}.
\]

\noindent $n = 4:$
\[
\Pr[\hlg(\bysinfer{\dataobs}, \ilapmech(\dataobs)) < 0.2851 ~ = LS(x)]
\geq 
\begin{cases}
 	0.87
	& \dataobs = (0, 4)\\
%
 	0.87
	& \dataobs = (1, 3)\\
%
	0.87
	& \dataobs = (3, 1)\\
%
	0.87
	& \dataobs = (4, 0)
\end{cases}.
\]

\[
\Pr[\hlg(\bysinfer{\dataobs}, \ilapmech(\dataobs)) < 0.2368 ~ = LS(x)]
\geq
\begin{cases}
0.74 & \dataobs = (2, 2)
\end{cases}
\]




$\dots$

\begin{thm}
To prove the optimality of Laplace mechanism, we are showing 
\[
\frac{ELap(\dataobs)}{(\epsilon*LS(\dataobs))}
\]
is $O(1)$, considering $n=|\dataobs| \geq 2$ being the parameter.

Where $LS(\cdot)$ is the local sensitivity, and where $ELap(\cdot)$ is the measure of the error of the Laplace mechanism, defined in this way:
\[
ELap(\dataobs) = \arg\big( \min\limits_{t}\{Pr[\hlg(\bysinfer(\dataobs), \ilapmech(\dataobs))< t] >= 0.748\} \big).
\]
\end{thm}

\begin{proof}
From the calculation above, we can conclude that the $ELap(\dataobs) = LS(\dataobs)$.\\
Then $\frac{ELap(\dataobs)}{(\epsilon*LS(\dataobs))} = \frac{1}{\epsilon} = O(1)$.

Given data set $\dataobs = (a,b)$ of size $ n = a + b \geq 2$, when apply the Laplace mechanism on it, we get the noised results $\dataobs'$. The probability that $\dataobs' = (a,b)$ or $(a + 1, b-1)$ or $(a - 1, b + 1)$ is:

{\scriptsize
\[
\begin{cases}
\left\{
\begin{array}{lr}
 	&
	0.5 \times 
 	(\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	1] 
 	 	+ 	
 	 	\Pr[	0	\leq \lap{0}{\frac{1}{\epsilon}}])\\
%
+	&
 	0.5 \times 
 	(\Pr[	1	\leq \lap{0}{\frac{1}{\epsilon}}	<	2] 
 	 	+ 	
 	 \Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}	<	0])\\
\end{array}
\right\}
	= 0.8742
	& a = 0,	b = n\\
%
\left\{
\begin{array}{lr}
 	&
 	0.5 \times 
 	(\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	0] 
 	+ 	
 	\Pr[	1	\leq \lap{0}{\frac{1}{\epsilon}}])\\
%
+	&
 	0.5 \times 
 	(\Pr[	0	\leq \lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 	 
 	\Pr[	0	\leq \lap{0}{\frac{1}{\epsilon}}	<	1] )\\
%
+ 	&
 	0.5 \times 
 	(\Pr[	1	\leq	\lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 	 
 	\Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}	<	0])\\
\end{array}
\right\}
 	= 0.8742
	& a = 1,	b = n - 1, n \geq 3\\
%
	1
	& a = 1, b = 1\\
%
\left\{ 	
\begin{array}{lr}
	&
	0.5 \times
	(
	\Pr[-1 \leq \lap{0}{\frac{1}{\epsilon}}	< 	0]
	+
	\Pr[1 \leq \lap{0}{\frac{1}{\epsilon}}	< 	2]
	)\\
+	&
	0.5 \times
	(
	\Pr[0 \leq \lap{0}{\frac{1}{\epsilon}}	< 	1]
	+
	\Pr[0 \leq \lap{0}{\frac{1}{\epsilon}}	< 	1]
	)\\
+	&
	0.5 \times
	(
	\Pr[1 \leq \lap{0}{\frac{1}{\epsilon}}]
	+
	\Pr[\lap{0}{\frac{1}{\epsilon}} < 0])
\end{array}
\right\}
	= 0.8742
	& a = n - 1,	b = 1, n \geq 3\\
%
\left\{
\begin{array}{lr}
 	&
 	0.5 \times 
 	(\Pr[	-1	\leq	\lap{0}{\frac{1}{\epsilon}}	<	0] 
 	+ 	
 	\Pr[	1	\leq	\lap{0}{\frac{1}{\epsilon}}	<	2])\\
+	&
 	0.5 \times 
 	(\Pr[	0	\leq	\lap{0}{\frac{1}{\epsilon}}	] 
 	  	+ 
 	  	\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	1]) 
\end{array}
\right\}
	= 0.8742
	& a = n,b = 0\\
%
\left\{
\begin{array}{lr}
 	&
 	0.5 \times 
 	(\Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}	<	0] 
 	+ 	
 	\Pr[	1	\leq \lap{0}{\frac{1}{\epsilon}}	<	2])\\
%
+	&
 	0.5 \times 
 	(\Pr[	0	\leq	\lap{0}{\frac{1}{\epsilon}}	<	1] 
 	+ 	
 	\Pr[	0	\leq \lap{0}{\frac{1}{\epsilon}}	<	1])\\
%
+	&
 	0.5 \times 
 	(\Pr[	1	\leq	\lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 	
 	\Pr[	-1 	\leq	\lap{0}{\frac{1}{\epsilon}}	<	0]) 
\end{array}
\right\}
	= 0.74839
	& o.w.\\
%
\end{cases}.
\]
}

Simplified:
{\footnotesize
\[
\begin{cases}
 	0.5 \times 
 	(\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 	
 	\Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}])
	= 0.8742
	& a = 0,	b = n\\
%
 	0.5 \times 
 	(\Pr[	\lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 	
 	\Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}])
 	= 0.8742
	& a = 1,	b = n - 1, n \geq 3\\
%
	= 1
	& a = 1, b = 1\\
%
 	0.5 \times 
 	(\Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}] 
 	+ 	
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}		<	2])
	= 0.8742
	& a = n - 1,	b = 1, n \geq 3\\
%
 	0.5 \times 
 	(\Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}] 
 	+ 	
 	\Pr[	\lap{0}{\frac{1}{\epsilon}}		<	2])
	= 0.8742
%
	& a = n,b = 0\\
 	0.5 \times 
 	(\Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}	<	2] 
 	+ 	
 	\Pr[	-1	\leq \lap{0}{\frac{1}{\epsilon}}	<	2])
	= 0.74839
	& o.w.
%
\end{cases}.
\]
}

By definition of adjacent data set, we know $\adj{\dataobs}{\dataobs'}$.\\
By definition of local sensitivity, we have:
\[
LS(\dataobs) = \max\limits_{\adj{\dataobs}{\dataobs''}}\hlg(\bysinfer{\dataobs},\bysinfer{\dataobs''})
\]
i.e., $\hlg(\bysinfer{\dataobs},\bysinfer{\dataobs'}) \leq LS(\dataobs)$.\\
Then, we can get:
\[
\{Pr[\hlg(\bysinfer{\dataobs}, \ilapmech(\dataobs)) < LS(\dataobs)] >= 0.748\}.
\]
Since the three $\dataobs'$ we are looking into is already the 3 nearest data set, so Hellinger distance between them are already the minimum distances. So, we can get: $ELap(\dataobs) = LS(\dataobs)$.\\
\[
\frac{ELap(\dataobs)}{(\epsilon*LS(\dataobs))} = \frac{1}{\epsilon} = O(1)
\]


\end{proof}



\end{document}
