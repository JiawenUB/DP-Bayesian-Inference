\documentclass{article}
\usepackage{caption}

\input{macros}
\usepackage{accents}

\begin{document}
\title{ Differentially Private Bayesian Inference}
%
\author{} % Author name
\date{}
\maketitle
%
\vspace{-50pt}
\section{Preliminary}
\[
\begin{array}{lcl}
	\vtheta 
	& : &
	\mbox{The parameter vector of multinomial distribution, $\vtheta\in [0,1]^{k}$}.\\
	%
	\dataobs 
	& : &
	\mbox{Observed dataset.}
	\dataobs \in \datauni^n, |\datauni| = k\\
	%
	\dirichlet{\valpha} 
	& : &
	\mbox{Dirichlet distribution. The prior or posterior distribution over $\vtheta$.}\\
	%
	\bysinfer{\valpha, \vtheta, \dataobs}
	& : & 
	\mbox{Posterior distribution over $\vtheta$ from Bayesian inference given prior distribution $\dirichlet{\valpha}$}\\
	%
	& &
	\mbox{and observed data set $\dataobs$.}\\
	%
	%
	\hellinger(\cdot, \cdot)
	& : &
   \mbox{Hellinger Distance between two distributions. }
   \hellinger(\dirichlet{\valpha_1}, \dirichlet{\valpha_2}) = \sqrt{1 - \frac{\betaf(\frac{\valpha_1 + \valpha_2}{2})}{\sqrt{\betaf(\valpha_1) \betaf(\valpha_2)}}} \\
   %
   %
   u(\dataobs, r)
   & : &
   \mbox{Scoring function given $\valpha$ and $\vtheta$ for candidate $r$. }
   u(\dataobs, r) =
   -\hellinger(\bysinfer{\valpha, \vtheta, \dataobs}, r)\\
   %
   GS
   & : & 
   \mbox{Global sensitivity of Hellinger distance. }
   GS = \sqrt{1 - \pi/4}\\
   %
   LS(\dataobs)
   & : & 
   \mbox{Local sensitivity of Hellinger distance for $\dataobs$. }\\
   %
   & &
   LS(\dataobs) = \max\limits_{\dataobs'\in\mathcal{X}^n,\adj{\dataobs}{\dataobs'}, r\in \candidateset{\valpha}}
   \lvert \hellinger(\bys{\dataobs}{\valpha}, r) - \hellinger(\bys{\dataobs'}{\valpha}, r)\rvert \\
   %
   S(\dataobs)
   & : & 
   \gamma-\mbox{smooth sensitivity. }
   S(\dataobs)=\max_{\dataobs' \in \mathcal{X}^{n}}\bigg \{ \frac{1}{\frac{1}{LS(\dataobs')} +\gamma \cdot Himming(\dataobs,\dataobs')} \bigg \}
\end{array}
\]
%
\section{Private Mechanisms}
%
%
%
\begin{algorithm}
  \caption{$\zlapmech$\cite{zhang2016differential} - Calibrating noise w.r.t. $\ell_1$ norm and Dimension}
  \label{lapmech}
  \begin{algorithmic}
  \STATE $\dataobs\in\mathcal{X}^n$, $\dirichlet{\valpha}$
  \STATE \quad {\bf let} $\valpha' = \inferd{\valpha}{\vtheta}{\dataobs}$
  \STATE \quad {\bf Initialize} a vector $\tilde{\valpha} = (0, \dots, 0 )\in \mathbb{N}^{|\mathcal{X}|}$ 
  \STATE \quad {\bf For} $i = 1 \dots |\mathcal{X}| - 1$:
  \STATE \quad \quad  {\bf let} $\tilde{\alpha_i}=\alpha_i + \lfloor{(\alpha_i' - \alpha_i)+ \lap{0}{\frac{2|\mathcal{X}|}{\epsilon}} }\rfloor^n_0$
  \STATE {\bf return} $\tilde{\valpha}$
  \end{algorithmic}
\end{algorithm}
%
%
%
\begin{algorithm}
  \caption{$\lapmech$ - Calibrating noise w.r.t. $\ell_1$ norm}
  \label{lapmech}
  \begin{algorithmic}
  \STATE $\dataobs\in\mathcal{X}^n$, $\dirichlet{\valpha}$
  \STATE \quad {\bf let} $\valpha' = \inferd{\valpha}{\vtheta}{\dataobs}$
  \STATE \quad {\bf Initialize} a vector $\tilde{\valpha} = (0, \dots, 0 )\in \mathbb{N}^{|\mathcal{X}|}$ 
  \STATE \quad {\bf For} $i = 1 \dots |\mathcal{X}| - 1$:
  % \STATE \quad \quad  {\bf let} $\eta \sim \lap{0}{\frac{|\mathcal{X}|}{\epsilon}}$
  \STATE \quad \quad  $\tilde{\alpha_i}=\alpha_i + \lfloor{(\alpha_i' - \alpha_i) + \lap{0}{\frac{|\mathcal{X}|}{\epsilon}}}\rfloor^n_0$ 
  % \RETURN $a_{k+1}$.
  \STATE \quad $\tilde{\alpha}_{|\mathcal{X}|} = \alpha_{|\mathcal{X}|} + \lfloor n - \sum_{i = 1}^{|\mathcal{X}|-1}\lfloor{(\alpha_i' - \alpha_i) + \eta_i}\rfloor^n_0 \rfloor^n_0$
  \STATE {\bf return} $\tilde{\valpha}$
  \end{algorithmic}
\end{algorithm}
%
%
%
%
%
%
\begin{algorithm}
  \caption{$\ilapmech$ - Calibrating noise w.r.t. histogram sensitivity}
  \label{ilapmech}
  \begin{algorithmic}
  \STATE {\bf input} $\dataobs\in\mathcal{X}^n$, $\dirichlet{\valpha}$
  \STATE \quad {\bf let} $\valpha' = \inferd{\valpha}{\vtheta}{\dataobs}$
  \STATE \quad ${\bf let}\ k=\left \{
        \begin{array}{lll}          
          1&\text{if}& |\mathcal{X}|=2\\
          2&\text{otherwise}
        \end{array}
      \right .\ $
  \STATE \quad {\bf Initialize} a vector $\tilde{\valpha} = (0, \dots, 0 )\in \mathbb{N}^{|\mathcal{X}|}$ 
  \STATE \quad {\bf For} $i = 1 \dots |\mathcal{X}|-1$:
  \STATE \quad \quad  {\bf let} $\eta \sim \lap{0}{\frac{k}{\epsilon}}$
  \STATE \quad \quad  $\tilde{\alpha_i}=\alpha_i + \lfloor{(\alpha_i' - \alpha_i) + \eta}\rfloor^n_0$ 
  % \RETURN $a_{k+1}$.
  \STATE \quad $\tilde{\alpha}_{|\mathcal{X}|} = \alpha_{|\mathcal{X}|} + \lfloor n - \sum_{i = 1}^{|\mathcal{X}|-1}\lfloor{(\alpha_i' - \alpha_i) + \eta_i}\rfloor^n_0 \rfloor^n_0$
  \STATE {\bf return} $\tilde{\valpha}$
  \end{algorithmic}
\end{algorithm}
%
%
%
%
% \begin{algorithm}
%   \caption{$\lapmech$}
%   \label{lapmech}
%   \begin{algorithmic}
%   	\REQUIRE $\dataobs \in \{0 ,1 \}^n$
% 	\STATE \quad apply the Bayesian inference algorithm on $\dataobs$, get true posterior $\betad(\valpha)$
%   	\STATE \quad {\bf let} $p = \uniform{0}{1}$, $\eta \sim \lap{0}{\frac{1.0}{\epsilon}}$
%   	\STATE \quad {\bf If} $p >0.5$: 
%   	\STATE \quad \quad with 0.5 probability adding noise to first component.
%   	\STATE \quad \quad  $\dataobs' = (\lfloor \text{x}_1 + \mu \rfloor_0^n, n - \lfloor \text{x}_1 + \mu \rfloor_0^n)$ 
%   	\STATE \quad {\bf Else}: 
%   	\STATE \quad \quad with 0.5 probability adding noise to second component.
%   	\STATE \quad \quad  $\dataobs' = (n - \lfloor \text{x}_2 + \mu \rfloor_0^n, \lfloor \text{x}_2 + \mu \rfloor_0^n)$ 
% 	\STATE \quad apply the Bayesian inference algorithm on $\dataobs'$, get: $\betad(\valpha')$
% 	\RETURN $\valpha'$
%   \end{algorithmic}
% \end{algorithm}
%
%
%
%
%
  \begin{algorithm}
  \caption{$\expmech$ - Instantiation
of the exponential mechanism}
  \label{mech:expmech1}
  \begin{algorithmic}
  \STATE observed data set $\dataobs \in \mathcal{X}^n$, prior: $\dirichlet{\valpha}$, $\epsilon$
  \STATE \quad {\bf let} $\dirichlet{\valpha'}= \bys{\dataobs}{\valpha}$.   
  \STATE \quad {\bf let} $GS$ be the global sensitivity for $\dataobs$.
  \STATE \quad {\bf set} $z=r$ with probability $\frac
  {\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r)}{2\cdot GS})}
{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r')}{2\cdot GS})}$
\STATE {\bf return} $z$
  \end{algorithmic}
  \end{algorithm}
%
%
%
%
%
  \begin{algorithm}
  \caption{$\lexpmech$ - Instantiation
	of the exponential mechanism with local sensitivity}
  \label{mech:lexpmech}
  \begin{algorithmic}
  \STATE {\bf input} observed data set $\dataobs \in \mathcal{X}^n$, prior: $\dirichlet{\valpha}$, $\epsilon$
  \STATE \quad {\bf let} $\dirichlet{\valpha'}= \bys{\dataobs}{\valpha}$.
  \STATE \quad {\bf let} $LS(\dataobs)$ be the local sensitivity for $\dataobs$.
  \STATE \quad {\bf set} $z=r$ with probability $\frac
  {\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r)}{2\cdot LS(\dataobs)})}
{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r')}{2\cdot LS(\dataobs)})}$
\STATE {\bf return} $z$
  \end{algorithmic}
  \end{algorithm}
%
%
  \begin{algorithm}
  \caption{$\hexpmech$ - Instantiation
	of the exponential mechanism with $\gamma$-smooth sensitivity}
  \label{mech:sexpmech}
  \begin{algorithmic}
  \STATE observed data set $\dataobs \in \mathcal{X}^n$, prior: $\dirichlet{\valpha}$, $\epsilon$
  \STATE \quad {\bf let} $\dirichlet{\valpha'}= \bys{\dataobs}{\valpha}$.   
  \STATE \quad {\bf let} $S(\dataobs)$ be the smooth sensitivity for $\dataobs$.
  \STATE \quad {\bf set} $z=r$ with probability $\frac{\exp(\frac{\epsilon\cdot \ux{r}}{2 (1 + \gamma) S(\dataobs)})}{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{\epsilon\cdot u(\dataobs, r')}{2 (1 + \gamma) S(\dataobs)})}$
\STATE {\bf return} $z$
  \end{algorithmic}
\end{algorithm}


\clearpage
\section{Privacy Analysis}
\begin{thm}
The $\lapmech$, $\ilapmech$, $\expmech$ and $\hexpmech$ are $\epsilon$-differentially private.
\end{thm}
The proofs are in the Arxiv version.

\clearpage
\section{Accuracy Analysis}
{\color{red}
\begin{thm}
To prove the optimality of Laplace mechanism, we are showing 
\[
\frac{ELap(\dataobs)}{(\epsilon \times LS(\dataobs))}
\]
is {\color{red}$O(1)$}, considering $n=|\dataobs| \geq 2$ being the parameter.

Where $LS(\cdot)$ is the local sensitivity, and where $ELap(\cdot)$ is the measure of the error of the Laplace mechanism, defined in this way:
 \[
ELap(\dataobs) = \arg\big( \min\limits_{t}\{Pr[\hlg(\bysinfer{\dataobs}, \ilapmech(\dataobs))< t] \geq 1 - \gamma \big).
\]
\end{thm}
}

\jiawen{
\begin{thm}
For $\gamma = e^{O(\epsilon)}$,
%
$\frac{ELap(\dataobs)}{(\epsilon \times LS(\dataobs))}$
is {\color{red}$O(\epsilon)$}
\end{thm}
}
%
\begin{proof}
%
Let $t = LS(\dataobs)$, we have following by p.d.f. of Laplace distribution:
 \[
Pr[\hlg(\bysinfer{\dataobs}, \ilapmech(\dataobs))< t] 
\geq 1 - \frac{1}{2}(e^{-\epsilon} + e^{-2\epsilon})
> 1 - e^{-\epsilon}
\]
Then we can get when $\gamma = e^{-\epsilon}$, 
\[
\frac{ELap(\dataobs)}{(\epsilon \times LS(\dataobs))} = \frac{1}{\epsilon}
\]
%
\end{proof}
%
%
%
{\color{red}
\begin{thm}
In order to prove the optimality of Laplace mechanism, instead of prove
$\frac{ELap(\dataobs)}{(\epsilon \times LS(\dataobs))}$ is $O(1)$, we prove a constant upper bound on following equations:
\[
\begin{array}{ll}
	 &	\frac{\arg\min\limits_{t}\big\{\Pr[\hlg(\betad(\alpha, \beta), \ilapmech(\dataobs))< t] \geq 1 - \gamma \big\}}
	 	{	LS(\dataobs)	}\\
\leq &	\frac{\max\limits_{|k| \leq \frac{\lg(\frac{1}{\gamma})}{\epsilon}}
		\hlg(\betad(\alpha, \beta), \betad(\alpha + k, n - \lfloor \alpha + k \rfloor))}
		{	LS(\dataobs)	}\\
\leq & 	O(\frac{\lg{\frac{1}{\gamma}}}{\epsilon})
\end{array}
\]
\end{thm}
}

\jiawen{
\begin{thm}
For $\gamma = e^{O(k) \epsilon}$, it is proved that $O(\frac{\lg{\frac{1}{\gamma}}}{\epsilon})$ is bounded by $O(k)$.
\end{thm}
}
%
%
\begin{proof}
By Laplace distribution, we have:
\[
\begin{array}{lcl}
	 \Pr[\hlg(\betad(\alpha, \beta), \ilapmech(\dataobs))< t]
	 & = & 
	 \Pr[\{|\lap{0}{\frac{1}{\epsilon}}|< O(k) |
	 \hlg(\betad(\alpha, \beta), \betad(\alpha + k, n - \lfloor \alpha + k \rfloor)) < t \}]\\
	 & \leq & 1 - e^{- O(k)\epsilon} \\
\end{array}
\]
Then we have:
\[
	\gamma = e^{- O(k)\epsilon}
\]
So we can get:
\[
	O(\frac{\lg{\frac{1}{\gamma}}}{\epsilon})
	= O(\frac{\lg{\frac{1}{e^{- O(k)\epsilon}}}}{\epsilon})
	= O(k)
\]
%
\end{proof}
%
%
%
\jiawen{
\begin{corollary}
For $-1 \leq k < 2$, it is proved that $O(\frac{\lg{\frac{1}{\gamma}}}{\epsilon})$ is bounded by $O(1)$.
\end{corollary}
}
%
%
\begin{proof}
Given $ -1 \leq k < 2$, we have: 
\begin{equation}
\label{eq:hls}
\hlg(\betad(\alpha, \beta), \betad(\alpha + k, n - \lfloor \alpha + k \rfloor)) \leq LS(\dataobs)
\end{equation}
For any $\epsilon$, $k \sim \lap{0}{\frac{1}{\epsilon}}$ from Laplace mechanism, we have:
\[
\Pr[|k| \leq \frac{b}{\epsilon}] = 1 - \exp(-b)
\]
Then we can get:
\begin{equation}
\label{eq:lap-1to2}
\Pr[ -1 \leq k < 2] = 1 - \frac{\exp(-\epsilon) + \exp(-2\epsilon)}{2}
\end{equation}
By Equation (\ref{eq:hls}) and (\ref{eq:lap-1to2}), we can get:
\[
\Pr[\hlg(\betad(\alpha, \beta), \betad(\alpha + k, n - \lfloor \alpha + k \rfloor)) \leq LS(\dataobs)] \geq 1 - \frac{\exp(-\epsilon) + \exp(-2\epsilon)}{2}
\]
i.e.,
\[
\begin{array}{ll}
	 &	\frac{\arg\min\limits_{t}\big\{\Pr[\hlg(\betad(\alpha, \beta), \ilapmech(\dataobs))< t] \geq 1 - \frac{\exp(-\epsilon) + \exp(-2\epsilon)}{2} \big\} }
	 {	LS(\dataobs)	}\\
\leq & 	O(\frac{\lg( \frac{2}{\exp(-\epsilon) + \exp(-2\epsilon)})}{\epsilon})\\
<    & O(\frac{\lg( \frac{2}{2 \exp(-2\epsilon)})}{\epsilon}) = 2
\end{array}
\]
\end{proof}

\jiawen{
\begin{thm}
Let $k = |k'|$ be the largest integer that satisfying $\hlg(\betad(\alpha, \beta), \betad(\alpha + k', n - \lfloor \alpha + k' \rfloor)) < t$, we have:
\[
  \Pr[\hlg(\betad(\alpha, \beta), \zlapmech(\dataobs))< t]
  = (1 - \frac{1}{2}(e^{-k\epsilon/2} + e^{-(k + 1)\epsilon/2})) ^2 \geq (1 - e^{-k\epsilon/2})^2
\]
%
\[
	\Pr[\hlg(\betad(\alpha, \beta), \ilapmech(\dataobs))< t]
  = 1 - \frac{1}{2}(e^{-k\epsilon} + e^{-(k + 1)\epsilon}) \geq 1 - e^{-k\epsilon}
\]
\[
	\frac{2ke^{-\epsilon} + 1}{n}
	<
	\Pr[\hlg(\betad(\alpha, \beta), \expmech(\dataobs)) < t]
	< 
	\frac{2k+1}{ne^{-\epsilon}}.
\]
\[
	\frac
  	{2k\exp(\frac{-\epsilon }{2\cdot LS(\dataobs)}) + 1}
	{n}
	<
	\Pr[\hlg(\betad(\alpha, \beta), \lexpmech(\dataobs)) < t]
	< 
	\frac
  	{2k + 1}
	{n\exp(\frac{-\epsilon }{2\cdot LS(\dataobs)})}.
\]
\end{thm}
}
\begin{proof}
Given $k = |k'|$ be the largest integer that satisfying $\hlg(\betad(\alpha, \beta), \betad(\alpha + k', n - \lfloor \alpha + k' \rfloor)) < t$, by the post-processing of Laplace distribution
and p.d.f. of Laplace distribution, we have:
%
\[
	\Pr[\hlg(\betad(\alpha, \beta), \ilapmech(\dataobs))< t]
	= \Pr[-k < \ilapmech(\dataobs) \leq k + 1] 
	= 1 - \frac{1}{2}(e^{-k\epsilon} + e^{-(k + 1)\epsilon})
  \geq 1 - e^{-k\epsilon}.
\]
%
%
By definition of $\expmech$ and $GS = \sqrt{1 - \pi/4}$, we have:
%
%
\[
	\begin{array}{lcl}
	Pr[\hlg(\betad(\alpha, \beta), \expmech(\dataobs)) < t] 
	& =  & \sum\limits_{c \geq - t}
	\frac
  {\exp(\frac{\epsilon\cdot c}{2\cdot GS})}
{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r')}{2\cdot GS})}\\
 	& \leq &
 	\frac
  	{2k\exp(-\frac{0\epsilon }{2\cdot GS}) + 1}
	{n\exp(\frac{-\epsilon }{2\cdot GS})}\\
 	& = &
 	\frac
  	{2k + 1}
	{n\exp(\frac{-\epsilon }{2\sqrt{1 - \pi/4}})}\\
	& < & 
 	\frac
  	{2k + 1}
	{n\exp(-\epsilon)}
\end{array}
\]
\[
	\begin{array}{lcl}
	Pr[\hlg(\betad(\alpha, \beta), \expmech(\dataobs)) < t] 
	& =  & \sum\limits_{c \geq - t}
	\frac
  	{\exp(\frac{\epsilon\cdot c}{2\cdot GS})}
	{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r')}{2\cdot GS})}\\
 	& \geq &
 	\frac
  	{2k\exp(\frac{-t\epsilon }{2\cdot GS}) + 1}
	{n\exp(\frac{-0\epsilon }{2\cdot GS})}\\
 	& > &
 	\frac
  	{2k\exp(\frac{-\epsilon }{2\sqrt{1 - \pi/4}}) + 1}
	{n}\\
	& > & 
 	\frac
  	{2ke^{-\epsilon} + 1}{n}
\end{array}
\]
%
%
By definition of $\lexpmech$, we have:
%
%
\[
	\begin{array}{lcl}
	Pr[\hlg(\betad(\alpha, \beta), \lexpmech(\dataobs)) < t] 
	& =  & \sum\limits_{c \geq - t}
	\frac
  	{\exp(\frac{c\epsilon}{2\cdot LS(\dataobs)})}
	{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r')}{2\cdot LS(\dataobs)})}\\
 	& \leq &
 	\frac
  	{2k\exp(\frac{-0\epsilon }{2\cdot LS(\dataobs)}) + 1}
	{n\exp(\frac{-\epsilon }{2\cdot LS(\dataobs)})}\\
 	& = &
 	\frac
  	{2k + 1}
	{n\exp(\frac{-\epsilon }{2\cdot LS(\dataobs)})}
\end{array}
\]
\[
	\begin{array}{lcl}
	Pr[\hlg(\betad(\alpha, \beta), \expmech(\dataobs)) < t] 
	& =  & \sum\limits_{c \geq - t}
	\frac
  {\exp(\frac{\epsilon\cdot c}{2\cdot LS(\dataobs)})}
{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r')}{2\cdot LS(\dataobs)})}\\
 	& \geq &
 	\frac
  	{2k\exp(\frac{-t\epsilon }{2\cdot LS(\dataobs)}) + 1}
	{n\exp(\frac{-0\epsilon }{2\cdot LS(\dataobs)})}\\
 	& > &
 	\frac
  	{2k\exp(\frac{-\epsilon }{2\cdot LS(\dataobs)}) + 1}
	{n}
\end{array}
\]
Since $\lim\limits_{n \rightarrow \infty} LS(\dataobs) \rightarrow 0$, 
we have $\lim\limits_{n \rightarrow \infty}\frac{-1}{LS(\dataobs)} \rightarrow -\infty$. So $\exp(\frac{-\epsilon }{2\cdot LS(\dataobs)})$ can only be bounded by $0$. We cannot found a tighter lower bound.
%
\end{proof}
%
%
\jiawen{
\begin{corollary}
For a reasonable small $t$, we have when data size $n = |\dataobs| > O(\frac{(2k + 1 )e^{\epsilon}}{1 - e^{-\epsilon}})$, the accuracy of $\ilapmech$ is higher than $\expmech$.
\end{corollary}
}
\begin{proof}
Based on Theorem 2.5, let:
\[
	\frac
  	{2k+1}
	{n\exp(-\epsilon)} \leq 1 - e^{-k\epsilon},
\]
we can have:
\[
	\Pr[\hlg(\betad(\alpha, \beta), \ilapmech(\dataobs))< t]
	> 
	\Pr[\hlg(\betad(\alpha, \beta), \expmech(\dataobs))< t].
\]
By simplification, we have $n > \frac{2k + 1}{e^{-\epsilon}(1 - e^{-k\epsilon})} \sim O(\frac{(2k + 1 )e^{\epsilon}}{1 - e^{-\epsilon}})$.
\end{proof}
%
%
\jiawen{
\begin{corollary}
For a reasonable small $t$, we have when data size $n = |\dataobs| < O(\frac{(2ke^{-\epsilon} + 1 )}{1 - \frac{1}{2}(e^{-k\epsilon} + e^{-(k + 1)\epsilon})})$, the accuracy of $\expmech$ is better than $\ilapmech$.
\end{corollary}
}
%
%
\begin{proof}
Applying the Theorem 2.5, let:
\[
  \frac
    {2ke^{-\epsilon}+1}
  {n} > 1 - \frac{1}{2}(e^{-k\epsilon} + e^{-(k + 1)\epsilon}),
\]
we can have:
\[
  \Pr[\hlg(\betad(\alpha, \beta), \ilapmech(\dataobs))< t]
  <
  \Pr[\hlg(\betad(\alpha, \beta), \expmech(\dataobs))< t].
\]
By simplification, we have $n < \frac{(2ke^{-\epsilon} + 1 )}{1 - \frac{1}{2}(e^{-k\epsilon} + e^{-(k + 1)\epsilon})}$.
\end{proof}
%
%
\jiawen{
\begin{corollary}
Let $R_{g}$ be the good output set where $\forall r \in R$, $\hlg(\bysinfer{\dataobs}, r) \leq LS(\dataobs)$, we have:
\[
	Pr[\ilapmech(\dataobs, \epsilon) \in R_{g}] > Pr[\expmech(\dataobs, \epsilon) \in R_{g}] 
\]
for data size $n = |\dataobs| > O(\frac{e^{\epsilon}}{1 - e^{-\epsilon}})$
\end{corollary}
}
%
\begin{proof}
simply apply the Theorem 2.5 and corollary 2.5.1, we can get this conclusion.


Let $R_{g}$ be the good output set where $\forall r \in R$, $\hlg(\bysinfer{\dataobs}, r) \leq LS(\dataobs)$, we have:

\[
	Pr[\ilapmech(\dataobs) \in R_{g}] 
	\geq 1 - \frac{1}{2}(e^{-\epsilon} + e^{-2\epsilon})
	> 1 - e^{-\epsilon}
\]
By definition of $\expmech$ and $GS = \sqrt{1 - \pi/4}$, we have:

\[
	\begin{array}{lcl}
	Pr[\expmech(\dataobs) \in R_{g}] 
	& =  & \sum\limits_{c \geq - LS(\dataobs)}
	\frac
  {\exp(\frac{\epsilon\cdot c}{2\cdot GS})}
{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r')}{2\cdot GS})}\\
 	& \leq &
 	\frac
  {2\exp(-\frac{\epsilon LS(\dataobs)}{2\cdot GS}) + 1}
{n\exp(\frac{-\epsilon }{2\cdot GS})}\\
 	& \leq &
 	\frac
  	{3}
	{n\exp(\frac{-\epsilon }{2\sqrt{1 - \pi/4}})}\\
	& \leq & 
 	\frac
  	{3}
	{n\exp(-\epsilon)}\\
\end{array}
\]

Let $c = 2\sqrt{1 - \pi/4}$, we have when $n > \frac{3}{e^{-\epsilon/c}(1 - e^{-\epsilon})} \sim O(\frac{e^{\epsilon}}{1 - e^{-\epsilon}})$ $\ilapmech$ performs better than $\expmech$.
\end{proof}

\small{\bibliographystyle{unsrt}
\bibliography{bayesian}}

\end{document}
