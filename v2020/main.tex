\documentclass{article}
\usepackage{caption}

\input{macros}
\usepackage{accents}

\begin{document}
\title{ Differentially Private Bayesian Inference \\
 Optimality of Laplace Mechanism}

\author{} % Author name

\maketitle

\section{Private Mechanisms}

\begin{algorithm}
  \caption{$\lapmech$}
  \label{lapmech}
  \begin{algorithmic}
  \STATE $\dataobs\in\mathcal{X}^n$, $\dirichlet{\valpha}$
  \STATE \quad {\bf let} $\valpha' = \inferd{\valpha}{\vtheta}{\dataobs}$
  \STATE \quad {\bf Initialize} a vector $\tilde{\valpha} = (0, \dots, 0 )\in \mathbb{N}^{|\mathcal{X}|}$ 
  \STATE \quad {\bf For} $i = 1 \dots |\mathcal{X}| - 1$:
  \STATE \quad \quad  {\bf let} $\eta \sim \lap{0}{\frac{|\mathcal{X}|}{\epsilon}}$
  \STATE \quad \quad  $\tilde{\alpha_i}=\alpha_i + \lfloor{(\alpha_i' - \alpha_i) + \eta}\rfloor^n_0$ 
  % \RETURN $a_{k+1}$.
  \STATE \quad $\tilde{\alpha}_{|\mathcal{X}|} = \alpha_{|\mathcal{X}|} + \lfloor n - \sum_{i = 1}^{|\mathcal{X}|-1}\lfloor{(\alpha_i' - \alpha_i) + \eta_i}\rfloor^n_0 \rfloor^n_0$
  \STATE {\bf return} $\tilde{\valpha}$
  \end{algorithmic}
\end{algorithm}
%
%
%
%
%
\begin{algorithm}
  \caption{$\lapmech$}
  \label{lapmech}
  \begin{algorithmic}
  	\REQUIRE $\dataobs \in \{0 ,1 \}^n$
	\STATE \quad apply the Bayesian inference algorithm on $\dataobs'$, get true posterior $\betad(\valpha)$
  	\STATE \quad {\bf let} $p = \uniform{0}{1}$, $\eta \sim \lap{0}{\frac{1.0}{\epsilon}}$
  	\STATE \quad {\bf If} $p >0.5$: 
  	\STATE \quad \quad with 0.5 probability adding noise to first component.
  	\STATE \quad \quad  $\dataobs' = (\lfloor \text{x}_1 + \mu \rfloor_0^n, n - \lfloor \text{x}_1 + \mu \rfloor_0^n)$ 
  	\STATE \quad {\bf Else}: 
  	\STATE \quad \quad with 0.5 probability adding noise to second component.
  	\STATE \quad \quad  $\dataobs' = (n - \lfloor \text{x}_2 + \mu \rfloor_0^n, \lfloor \text{x}_2 + \mu \rfloor_0^n)$ 
	\STATE \quad apply the Bayesian inference algorithm on $\dataobs'$, get: $\betad(\valpha')$
	\RETURN $\valpha'$
  \end{algorithmic}
\end{algorithm}
%
%
%
%
%
  \begin{algorithm}
  \caption{$\expmech$}
  \label{mech:expmech1}
  \begin{algorithmic}
  \STATE observed data set $\dataobs \in \mathcal{X}^n$, prior: $\dirichlet{\valpha}$, $\epsilon$
  \STATE \quad {\bf let} $\dirichlet{\valpha'}= \bys{\dataobs}{\valpha}$.   
  \STATE \quad {\bf let} $GS$ be the global sensitivity for $\dataobs$.
  \STATE \quad {\bf set} $z=r$ with probability $\frac
  {\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r)}{2\cdot GS})}
{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r')}{2\cdot GS})}$
\STATE {\bf return} $z$
  \end{algorithmic}
  \end{algorithm}
%
%
%
%
%
  \begin{algorithm}
  \caption{$\lexpmech$}
  \label{mech:lexpmech}
  \begin{algorithmic}
  \STATE {\bf input} observed data set $\dataobs \in \mathcal{X}^n$, prior: $\dirichlet{\valpha}$, $\epsilon$
  \STATE \quad {\bf let} $\dirichlet{\valpha'}= \bys{\dataobs}{\valpha}$.
  \STATE \quad {\bf let} $LS(\dataobs)$ be the local sensitivity for $\dataobs$.
  \STATE \quad {\bf set} $z=r$ with probability $\frac
  {\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r)}{2\cdot LS(\dataobs)})}
{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r')}{2\cdot LS(\dataobs)})}$
\STATE {\bf return} $z$
  \end{algorithmic}
  \end{algorithm}
%
%
  \begin{algorithm}
  \caption{$\hexpmech$}
  \label{mech:sexpmech}
  \begin{algorithmic}
  \STATE observed data set $\dataobs \in \mathcal{X}^n$, prior: $\dirichlet{\valpha}$, $\epsilon$
  \STATE \quad {\bf let} $\dirichlet{\valpha'}= \bys{\dataobs}{\valpha}$.   
  \STATE \quad {\bf let} $S(\dataobs)$ be the smooth sensitivity for $\dataobs$.
  \STATE \quad {\bf set} $z=r$ with probability $\frac{\exp(\frac{\epsilon\cdot \ux{r}}{4\cdot S(\dataobs)})}{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{\epsilon\cdot u(\dataobs, r')}{4\cdot S(\dataobs)})}$
\STATE {\bf return} $z$
  \end{algorithmic}
\end{algorithm}



\clearpage
\section{Accuracy Analysis}
{\color{red}
\begin{thm}
Let $R_{g}$ be the good output set where $\forall r \in R$, $\hlg(\bysinfer{\dataobs}, r) \leq LS(\dataobs)$, we have:
\[
	Pr[\ilapmech(\dataobs, \epsilon) \in R_{g}] > Pr[\expmech(\dataobs, \epsilon) \in R_{g}] 
\]
for data size $n = |\dataobs| > O(\frac{e^{\epsilon}}{1 - e^{-\epsilon}})$
\end{thm}
}
%
Let $R_{g}$ be the good output set where $\forall r \in R$, $\hlg(\bysinfer{\dataobs}, r) \leq LS(\dataobs)$, we have:

\[
	Pr[\ilapmech(\dataobs) \in R_{g}] 
	\geq 1 - \frac{1}{2}(e^{-\epsilon} + e^{-2\epsilon})
	> 1 - e^{-\epsilon}
\]
By definition of $\expmech$ and $GS = \sqrt{1 - \pi/4}$, we have:

\[
	\begin{array}{lcl}
	Pr[\expmech(\dataobs) \in R_{g}] 
	& =  & \sum\limits_{c \geq - LS(\dataobs)}
	\frac
  {\exp(\frac{\epsilon\cdot c}{2\cdot GS})}
{\Sigma_{r' \in \candidateset{\valpha}}\exp(\frac{-\epsilon\cdot \hellinger(\bys{\dataobs}{\valpha}, r')}{2\cdot GS})}\\
 	& \leq &
 	\frac
  {2\exp(-\frac{\epsilon LS(\dataobs)}{2\cdot GS}) + 1}
{n\exp(\frac{-\epsilon }{2\cdot GS})}\\
 	& \leq &
 	\frac
  	{3}
	{n\exp(\frac{-\epsilon }{2\sqrt{1 - \pi/4}})}\\
	& \leq & 
 	\frac
  	{3}
	{n\exp(-\epsilon)}\\
\end{array}
\]

Let $c = 2\sqrt{1 - \pi/4}$, we have when $n > \frac{3}{e^{-\epsilon/c}(1 - e^{-\epsilon})} \sim O(\frac{e^{\epsilon}}{1 - e^{-\epsilon}})$ $\ilapmech$ performs better than $\expmech$.

{\color{red}
\begin{thm}
To prove the optimality of Laplace mechanism, we are showing 
\[
\frac{ELap(\dataobs)}{(\epsilon \times LS(\dataobs))}
\]
is {\color{red}$O(\epsilon)$}, considering $n=|\dataobs| \geq 2$ being the parameter.

Where $LS(\cdot)$ is the local sensitivity, and where $ELap(\cdot)$ is the measure of the error of the Laplace mechanism, defined in this way:
 \[
ELap(\dataobs) = \arg\big( \min\limits_{t}\{Pr[\hlg(\bysinfer{\dataobs}, \ilapmech(\dataobs))< t] \geq 1 - \gamma \big).
\]
\end{thm}
}
%
\begin{proof}
%
Let $t = LS(\dataobs)$, we have following by p.d.f. of Laplace distribution:
 \[
Pr[\hlg(\bysinfer{\dataobs}, \ilapmech(\dataobs))< t] 
\geq 1 - \frac{1}{2}(e^{-\epsilon} + e^{-2\epsilon})
> 1 - e^{-\epsilon}
\]
Then we can get when $\gamma = e^{-\epsilon}$, 
\[
\frac{ELap(\dataobs)}{(\epsilon \times LS(\dataobs))} = \frac{1}{\epsilon}
\]
%
\end{proof}
%
%
%
{\color{red}
\begin{thm}
In order to prove the optimality of Laplace mechanism, instead of prove
$\frac{ELap(\dataobs)}{(\epsilon \times LS(\dataobs))}$ is $O(1)$, we prove a constant upper bound on following equations:
\[
\begin{array}{ll}
	 &	\frac{\arg\min\limits_{t}\big\{\Pr[\hlg(\betad(\alpha, \beta), \ilapmech(\dataobs))< t] \geq 1 - \gamma \big\}}
	 	{	LS(\dataobs)	}\\
\leq &	\frac{\max\limits_{|k| \leq \frac{\lg(\frac{1}{\gamma})}{\epsilon}}
		\hlg(\betad(\alpha, \beta), \betad(\alpha + k, n - \lfloor \alpha + k \rfloor))}
		{	LS(\dataobs)	}\\
\leq & 	O(\frac{\lg{\frac{1}{\gamma}}}{\epsilon})
\end{array}
\]
\end{thm}
}

\begin{proof}
By Laplace distribution, we have:
\[
\begin{array}{lcl}
	 \Pr[\hlg(\betad(\alpha, \beta), \ilapmech(\dataobs))< t]
	 & = & 
	 \Pr[\{|\lap(\frac{1}{\epsilon}|< O(k) |
	 \hlg(\betad(\alpha, \beta), \betad(\alpha + k, n - \lfloor \alpha + k \rfloor)) < t \}]\\
	 & \leq & 1 - e^{- O(k)\epsilon} \\
\end{array}
\]
Then we have:
\[
	\gamma = e^{- O(k)\epsilon}
\]
So we can get:
\[
	O(\frac{\lg{\frac{1}{\gamma}}}{\epsilon})
	= O(\frac{\lg{\frac{1}{e^{- O(k)\epsilon}}}}{\epsilon})
	= O(k)
\]

\end{proof}
%
\begin{proof}
By setting $ -1 \leq k < 2$, we have: 
\begin{equation}
\label{eq:hls}
\hlg(\betad(\alpha, \beta), \betad(\alpha + k, n - \lfloor \alpha + k \rfloor)) \leq LS(\dataobs)
\end{equation}
For any $\epsilon$, $k \sim \lap{0}{\frac{1}{\epsilon}}$ from Laplace mechanism, we have:
\[
\Pr[|k| \leq \frac{b}{\epsilon}] = 1 - \exp(-b)
\]
Then we can get:
\begin{equation}
\label{eq:lap-1to2}
\Pr[ -1 \leq k < 2] = 1 - \frac{\exp(-\epsilon) + \exp(-2\epsilon)}{2}
\end{equation}
By Equation (\ref{eq:hls}) and (\ref{eq:lap-1to2}), we can get:
\[
\Pr[\hlg(\betad(\alpha, \beta), \betad(\alpha + k, n - \lfloor \alpha + k \rfloor)) \leq LS(\dataobs)] \geq 1 - \frac{\exp(-\epsilon) + \exp(-2\epsilon)}{2}
\]
i.e.,
\[
\begin{array}{ll}
	 &	\frac{\arg\min\limits_{t}\big\{\Pr[\hlg(\betad(\alpha, \beta), \ilapmech(\dataobs))< t] \geq 1 - \frac{\exp(-\epsilon) + \exp(-2\epsilon)}{2} \big\} }
	 {	LS(\dataobs)	}\\
\leq & 	O(\frac{\lg( \frac{2}{\exp(-\epsilon) + \exp(-2\epsilon)})}{\epsilon})\\
<    & O(\frac{\lg( \frac{2}{2 \exp(-2\epsilon)})}{\epsilon}) = 2
\end{array}
\]
\end{proof}




\end{document}
