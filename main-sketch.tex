\documentclass[sigconf, anonymous]{acmart}

\fancyhf{} % Remove fancy page headers 
\fancyhead[C]{Anonymous submission \#9999 to ACM CCS 2017} % TODO: replace 9999 with your paper number
\fancyfoot[C]{\thepage}

\setcopyright{none} % No copyright notice required for submissions
\acmConference[Anonymous Submission to ACM CCS 2017]{ACM Conference on Computer and Communications Security}{Due 19 May 2017}{Dallas, Texas}
\acmYear{2017}

\settopmatter{printacmref=false, printccs=true, printfolios=true} % We want page numbers on submissions

%%\ccsPaper{9999} % TODO: replace with your paper number once obtained
\input{macros}

\begin{document}
\title{Sketch of DP - Bayesian Inference} % TODO: replace with your title

\begin{abstract}
Bayesian inference is a statistical method which allows to derive a
\emph{posterior} distribution over a parameter, starting from a
\emph{prior} distribution and observed data.  In a setting where the
data must be protected, different approaches have been taken to make
this process differentially private. \citet{dimitrakakis2014robust},
and \citet{wang2015privacy}, for instance, proved that, under specific
conditions, sampling from the posterior distribution is already
differentially private. Other works,
e.g. \cite{zhang2016differential}, \cite{foulds2016theory}, designed
differentially private mechanisms that output a representation of the
full posterior distribution. Also, accuracy of the mechanisms was
measured using metrics on the space of the numeric parameters of the
posterior distribution, e.g. $\ell_1$-norm .

In this work we present a new differentially private algorithm to
compute the posterior in a Beta-Binomial system and in Dirichlet-Multinomial system.
The noise is calibrated w.r.t. a smooth
sensitivity computed with a metric on probability distributions; we
measured the accuracy of the process by using a metric over
probability measures. We compared the accuracy of this approach with
the one based on $\ell_1$-norm. Experimental results show that when
the data size is small or when the parameters of the prior
distribution are large, the former outperforms the latter.

\end{abstract}


% TODO: replace this section with code generated by the tool at https://dl.acm.org/ccs.cfm
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002978.10003029.10011703</concept_id>
<concept_desc>Security and privacy~Usability in security and privacy</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc{Security and privacy~Use https://dl.acm.org/ccs.cfm to generate actual concepts section for your paper}
% -- end of section to replace with generated code

\keywords{differential privacy, Bayesian inference, Hellinger distance} % TODO: replace with your keywords

\maketitle

% \input{main-sketch} % TODO: replace with your brilliant paper!
% \input{macros}

\section{Bayesian Inference Background}
\label{sec_background}
Given a prior belief $\Pr(\theta)$ on some parameter $\theta$,
and given an observation $\dataobs$,
the posterior distribution on $\theta$ given $\dataobs$ is computed as:
\[
  \Pr(\theta | \dataobs) = \frac{\Pr(\dataobs | \theta) \cdot \Pr(\theta)}{\Pr(\dataobs)}
\]
where the expression  $\Pr(\dataobs | \theta)$ denotes the \emph{likelihood} of $\theta$ when
$\dataobs$ is observed. Since we consider $\dataobs$ to be fixed the likelihood is a function of $\theta$.
For the same reason $\Pr(\dataobs)$ is a constant independent of $\theta$.
Usually in statistics the prior distribution $\Pr(\theta)$ is chosen so that it represents
the initial belief on $\theta$. That is, when no data has been observed. In practice though,
prior distributions and likelihood functions are usually chosen so that the posterior
belongs to the same \emph{family} of distributions. In this case we say that the prior
is conjugate of the likelihood function. Using conjugate priors simplify calculcations and allows
for inference to be performed in a recursive fashion over the data.
In this work we will consider a specific instance of Bayesian inference and one of its generalizations.
Specifically, we will consider the situation where $\theta$ represents the parameter --informally called \emph{bias}--
of a Bernoulli distributed random variable, and its immediate generalization where the parameter $\vtheta$ represents
the the vector of parameters of a categorical distributed random variable.
In the former case the prior distribution over $\theta\in [0,1]$ is going to be a beta distribution, $\betad(\alpha, \beta)$, with parameters
$\alpha,\beta\in\mathbb{R}^{+}$, and with p.d.f:
\[
  \Pr(\theta)\equiv \frac{\theta^{\alpha} (1- \theta)^{\beta}}{\betaf(\alpha,\beta)}
\]
where $\betaf(\cdot,\cdot)$ is the beta function.
The data $\dataobs$ will be a sequence of $n\in\mathbb{N}$ 0/1 values, that is $\dataobs=\ll x_1,\dots x_n \gg, x_i\in\{0,1\}$, and the likelihood function is:
\[
  \Pr(\dataobs | \theta)\equiv \theta^{\Delta \alpha}(1-\theta)^{n - \Delta \alpha}
\]
where $\Delta \alpha = \displaystyle\sum_{i=1}^{n}x_i$.
From this it can easily be derived that the posterior distribution is:
\[
  \Pr(\theta|\dataobs)=\betad(\alpha + \Delta \alpha,\beta + n - \Delta \alpha)
\]
In the latter case the prior distribution over $\vtheta\in [0,1]^{k}$ is given by a Dirichelet distribution, $\dirichlet(\valpha)$, for $k\in\mathbb{N}$,
and $\valpha\in(\mathbf{R^{+}})^{k}$, with p.d.f:
\[
  \Pr(\vtheta)\equiv\frac{1}{\mbetaf(\valpha)}\cdot \displaystyle\prod_{i=1}^{k}{\theta_i^{\alpha_i-1}}
\]
where $\mbetaf(\cdot)$ is the generealised beta function.
The data $\dataobs$ will be a sequence of $n\in\mathbb{N}$ values coming from a universe $\datauni$, such that $\mid\datauni \mid=k$.
The likelihood function will be:
\[
  \Pr(\dataobs|\vtheta)\equiv \displaystyle\prod_{a_i\in\datauni}\theta_{i}^{\Delta \alpha_i},
\]
where $\Delta \alpha_i=\displaystyle\sum_{i=1}^{n}\iverson{x_i=a_i}$, where in $\iverson{\cdot}$ we use
Iverson bracket notation. The posterior distribution over $\vtheta$ turns out to be
\[
  \Pr(\vtheta|\dataobs)=\dirichlet(\valpha+\Delta \valpha). 
\]
where $+$ denotes the component wise  sum of vectors of reals. 


\section{The Problem and Baseline approach}
We are interested in designing a mechanism for privately releasing
\emph{fully} the posterior distributions derived in section \ref{sec_background},
and not a sample from them.  It's worth noticing that the posterior
distributions are fully characterized by their parameters, and the
family (beta, Dirichlet) they belong to.  Hence, in case of the
Beta-Binomial model we are interested in releasing a private version
of the pair of parameters $(\alpha',\beta')=(\alpha + \Delta \alpha,\beta + n - \Delta \alpha)$, and
in the case of the Dirichlet-Multinomial model we are interested in a
private version of $\valpha'=(\valpha + \Delta \valpha)$. \citet{zhang2016differential} and \citet{xiao2012bayesian} have
already attacked this problem and their solution consisted in adding
independent Laplacian noise to the parameters of the posteriors. That
is, in the case of the Beta-Binomial system the value released would
be: $(\tilde\alpha,\tilde\beta)=(\alpha + \Delta \tilde\alpha,\beta + n - \Delta \tilde\alpha)$ where $\Delta \tilde\alpha\sim Lap(\Delta \alpha, \frac{2}{\epsilon})$, and
%with truncating method come from algorithm designed by \citet{zhang2016differential}
where $Lap(\mu,\nu)$
denotes a Laplacian random variable with mean $\mu$ and scale $\nu$.
This mechanism is $\epsilon$-differentially private, and the noise is
calibrated w.r.t to a sensitivity of 2 which is derived by using
$\ell_1$ norm over the pair of parameters. Indeed, considering two
adjacent\footnote{Given $\dataobs, \dataobs'\in\{0,1\}^{n}$ we say that $\dataobs$ and $\dataobs'$ are adjacent and we write, $\adj{\dataobs}{\dataobs'}$, iff
$\displaystyle \sum_{i}^{n}\iverson{x_i = x'_i }\leq 1$ } data observations
$\dataobs, \dataobs'$, that, from a unique prior, give rise to two posterior
distributions, characterized by the pairs
$(\alpha',\beta')$ and $(\alpha'',\beta'')$ then
$|\alpha'-\alpha''|+|\beta'-\beta''|\leq 2$.
This argument extends similarly to the Dirichelet-Multinomial system.


\noindent Also, in previous works, the accuracy has been measured again with respect to
$\ell_1$ norm. That is, a lower bound has been given to
\[
  \Pr[|\alpha - \tilde\alpha| + |\beta - \tilde\beta |\geq \gamma ]
\]
where $(\alpha, \beta), (\tilde\alpha,\tilde\beta)$ are as  defined above.
In this work we will use a metric based on a different norm to compute the sensitivity
and provide guarantees on the accuracy. In particular we will consider a metric
over probability measures and not over the parameters that represent them.
Specifically, we will use the Hellinger distance $\hellinger(\cdot,\cdot)$, which for two beta distributions
$\boldsymbol{\beta}_1=\betad(\alpha_1, \beta_1),$ and $\boldsymbol{\beta}_2=\betad(\alpha_2, \beta_2)$ is equal to
\[
  \hellinger(\boldsymbol{\beta}_1, \boldsymbol{\beta}_2)\equiv
  \sqrt{1 - \frac{\betaf(\frac{\alpha_1 + \alpha_2}{2}, \frac{\beta_1 + \beta_2}{2})}{\sqrt{\betaf(\alpha_1,\beta_1)\betaf(\alpha_2,\beta_2)}}}
\]
The same change of metric will be applied to the experimental accuracy guarantees.
% \section{Technical Problem Statement}
% \label{sec_bayesInfer}
% The posterior belief about parameters $\xis$ is inferred from the
% group of sensitive data. Because it is usually published in form of a
% distribution, for example, $\betad(\alpha',\beta'), \cdots,$ itself
% can relieve a lot of information about sensitive data. So, the
% sensitive information we want to protect here is the posterior belief,
% i.e., $Pr(\xis | \dataobs)$, rather than just a sample drew from
% it\cite{foulds2016theory}. Then it will be better if we can use a
% distance over two distributions to measure the accuracy than a
% distance over just parameters or values when we are protecting a
% distribution, for instance, Hellinger distance in our paper. When we
% measure the accuracy between true posterior and protected posterior
% using Hellinger distance, it would be better we can have a protection
% mechanism based on Hellinger distance. However, current mechanisms are
% mainly based on $l_1$ norm by Laplace mechanism
% \cite{zhang2016differential} \cite{xiao2012bayesian}. That's why we
% design our exponential mechanism over Hellinger distance to protect
% the whole posterior distribution.


% \section{Baseline Approach - Laplace Mechanism}
% \label{sec_lap}
% Our baseline approach come from the algorithm designed by \citet{zhang2016differential} that adding Laplace noise (\citet{dwork2014algorithmic}) to posterior parameters to preserve privacy. They perturbed the parameter updates of $\betad$ distribution from Bayesian inference and truncated to rule out invalid $\betad$ parameters to get the protected posteriors.
% Furthermore, we extend their algorithm to the case of $m$ dimensional Dirichlet distributions, where we add $(m-1)$ i.i.d. Laplace noises $\{Lap_1 , Lap_2, \cdots, Lap_{m-1} \}$ and truncate in the same way. 
% %where $Lap_i = floor(Y)$, $Y \sim Lap(\frac{2}{\epsilon})$. 
% The private posterior is $\dirichlet(\valpha + \Delta \valpha')$, where $\Delta \valpha'$ is the perturbed parameter updates.

\section{Our Approach - Exponential Mechanism with Smooth Sensitivity}
\label{sec_smoo}
Given a prior distribution $\bprior=\betad(\alpha, \beta)$ and a sequence of $n$ observations $\dataobs\in\{0,1\}^n$,
we define the follwing set:

\[
  \betaset\equiv\{\betad(\alpha',\beta')\mid \alpha'=\alpha+\Delta\alpha, \beta'=\beta+n-\Delta\alpha\}
\]
where $\Delta\alpha$ is as defined in Section \ref{sec_background}.
Notice that $\betaset$ has $n+2$ elements, and the Bayesian Inference
process will produce an element from $\betaset$ that we denote by
$\bysinfer(\dataobs)$ -- we don't explicitely parametrize the result by the
prior,  which from now on we consider fixed and we denote it by $\bprior$.


We can now define the mechanism $\hexpmech$ which,
given in input a sequence of observations $\dataobs$, and $\epsilon>0, \delta>0$,
produces an element $r$ in $\betaset$ with probability: 

\begin{equation*}
\underset{z \thicksim \hexpmech}{\Pr}[z=r] = \frac {exp(\frac{-\epsilon\cdot\hellinger(\bysinfer(\dataobs),r)}{2\cdot S_\beta(\dataobs)})}
{\displaystyle\sum_{r\in\betaset} exp(\frac{-\epsilon\cdot\hellinger(\bysinfer(\dataobs),r)}{2\cdot S_{\beta}(\dataobs)})}
\end{equation*}
this mechanism is based on the basic exponential mechanism \cite{talwar}, with $\betaset$ as range and $\hellinger(\cdot,\cdot)$ as scoring function.
The difference is that in this mechanism we don't calibrate the noise w.r.t to the global sensitivity of the scoring function but w.r.t to the smooth sensitivity
-- \citet{nissim2007smooth}-- of $\hellinger(\bysinfer(\dataobs), \cdot)$, which is :
\[
   S_{\beta}(\dataobs)=\max_{\dataobs' \neq \dataobs, \dataobs' \in \{0,1\}^{n}}\bigg \{\Delta_{l}\bigg (\hellinger(\bysinfer(\dataobs'),\cdot)\bigg )\cdot e^{-\gamma\cdot d(\dataobs, \dataobs')}\bigg\}
 \]

 where $d$ is the Hamming distance between two datasets,  $\gamma = \gamma(\epsilon, \delta)$ is a function
 of $\epsilon$ and $\delta$ to be determined later, and where $\Delta_{l}\bigg (\hellinger(\bysinfer(\dataobs'),\cdot)\bigg )$
 denotes the local sensitivity at $\bysinfer(\dataobs')$, or equivalently at $\dataobs'$ --w.r.t $\ell_1$ norm--
 of the scoring function used in our mechanism, that is:
 \begin{equation*}
 \Delta_{l}\bigg (\hellinger(\bysinfer(\dataobs'),\cdot)\bigg )=\max_{\dataobs'' \in \datauni^n:\adj{\dataobs'}{\dataobs''}, r\in \betaset}\lvert \hellinger(\bysinfer(\dataobs'), r) - \hellinger(\bysinfer(\dataobs''), r)\rvert
\end{equation*}

\subsection{Analysis of the mechanism}
In what follows, we will use a correspondence between the probability
 $\underset{z \thicksim \hexpmech(x)}{Pr}[z = r]$ of every
 $r\in\betaset$ and the probability 
 $\underset{z \thicksim \hexpmech(x)}{Pr}[\hlg(\bysinfer(x),z) =
 \hlg(\bysinfer(x),r)]$ for the utility score for $r$. In particular, for every
 $r\in\betaset$ we have:
$$
\underset{z \thicksim \hexpmech(x)}{Pr}[z = r]=
\frac{1}{2}\Big (\underset{z \thicksim \hexpmech(x)}{Pr}[\hlg(\bysinfer(x),z) =
 \hlg(\bysinfer(x),r)]\Big )
$$
To see this, it is enough to notice that: $\underset{z \thicksim \hexpmech(x)}{Pr}[z = r]$ is proportional too $\hlg(\bysinfer(x),r)$, i.e., $u(x,z)$. We can derive, if $u(r,x) = u(r',x)$ then $\underset{z \thicksim \hexpmech(x)}{Pr}[z = r] = \underset{z \thicksim \hexpmech(x)}{Pr}[z = r']$. We assume the number of candidates $z \in \mathcal{R}$ that satisfy $u(z,x) = u(r,x)$ is $|r|$, we have  $\underset{z \thicksim \hexpmech(x)}{Pr}[u(z,x) = u(r,x)] = |r| \underset{z \thicksim \hexpmech(x)}{Pr}[z = r]$. Because Hellinger distance  $\hlg(\bysinfer(x),z)$ is axial symmetry, where the $\bysinfer(x)$ is the symmetry axis. It can be infer that $|z| = 2$ for any candidates, apart from the true output, i.e., $\underset{z \thicksim \hexpmech(x)}{Pr}[u(z,x) = u(r,x)] = 2 \underset{z \thicksim \hexpmech(x)}{Pr}[z = r]$. This parameter can be eliminate in both sides in proof.

In our private Bayesian inference mechanism, we set the $\beta$ as $\ln(1 - \frac{\epsilon}{2 \ln (\frac{\delta}{2 (n + 1)})})$. 





% \subsection{Sliding Property of Exponential Mechanism}
% \begin{lem}
% Consider the exponential mechanism  $\sexpmech(x,u,\mathcal{R})$
% calibrated on the smooth sensitivity. Let $\lambda = f(\epsilon,
% \delta)$, $\epsilon\geq 0$ and $|\delta| < 1$. Then, the following \emph{sliding property} holds:
% \begin{equation*}
% \underset{r \thicksim \hexpmech(x)}{Pr}[u(r,x) = \hat{s}]
% \leq
% e^{\frac{\epsilon}{2}} \underset{r \thicksim \hexpmech(x)}{Pr}[u(r,x) = (\Delta + \hat{s})] + \frac{\delta}{2},
% \end{equation*}

% \end{lem}

% \begin{proof}

% We denote the normalizer of the probability mass in $\hexpmech(x)$: $\sum_{r' \in \mathcal{R}}exp(\frac{\epsilon u(r',x)}{2 S(x)})$ as $NL(x)$:
% \begin{equation*}
% \begin{split}
% LHS 
%   = \underset{r \thicksim \hexpmech(x)}{Pr}[u(r,x) = \hat{s}]
% & = \frac{exp(\frac{\epsilon \hat{s}}{2 S(x)})}{NL(x)}\\
% & = \frac{exp(\frac{\epsilon (\hat{s} + \Delta - \Delta)}{2 S(x)})}{NL(x)}\\
% & = \frac{exp(\frac{\epsilon (\hat{s} + \Delta)}{2 S(x)} + \frac{- \epsilon \Delta}{2 S(x)})}{NL(x)}\\
% & = \frac{exp(\frac{\epsilon (\hat{s} + \Delta)}{2 S(x)})}{NL(x)} \cdot e^{\frac{- \epsilon \Delta}{2 S(x)})}.\\
% \end{split}
% \end{equation*}

% By bounding the $\Delta \geq -S(x)$, we can get:

% \begin{equation*}
% \begin{split}
% \frac{exp(\frac{\epsilon (\hat{s} + \Delta)}{2 S(x)})}{NL(x)} \cdot e^{\frac{- \epsilon \Delta}{2 S(x)}}
% & \leq \frac{exp(\frac{\epsilon (\hat{s} + \Delta)}{2 S(x)})}{NL(x)} \cdot e^{\frac{\epsilon}{2}}\\
% &  =  e^{\frac{\epsilon}{2}} \underset{z \thicksim \hexpmech(x)}{Pr}[u(r,x) = (\Delta + \hat{s})] \leq RHS\\
% \end{split}
% \end{equation*}

% \end{proof}

% \subsection{Dilation Property of Exponential Mechanism}
% \begin{lem}
% for any exponential mechanism $\hexpmech(x)$, $\lambda < |\beta|$, $\epsilon$, $|\delta| < 1$ and $\beta \leq \ln(1 - \frac{\epsilon}{2 \ln (\frac{\delta}{2 (n + 1)})})$, the dilation property holds:

% \begin{equation*}
% \underset{z \thicksim \hexpmech(x)}{Pr}[u(z,x) = c]
% \leq
% e^{\frac{\epsilon}{2}} \underset{z \thicksim \hexpmech(x)}{Pr}[u(z,x) = e^{\lambda} c] + \frac{\delta}{2},
% \end{equation*}
% where the sensitivity in mechanism is still smooth sensitivity as above.
% \end{lem}

% \begin{proof}

% The sensitivity is always greater than 0, and our utility function $-\hlg(\bysinfer(x),z)$ is smaller than zero, i.e., $u(z,x) \leq 0$, we need to consider two cases where $\lambda < 0$, and $\lambda > 0$:

% We set the $h(c) = Pr[u(\hexpmech(x)) = c] = 2\frac{exp(\frac{\epsilon z}{2 S(x)})}{NL(x)}$.

% We first consider $\lambda < 0$. In this case, $1 < e ^ {\lambda}$, so the ratio $\frac{h(c)}{h(e^{\lambda}c)} = \frac{exp(\frac{\epsilon c}{2 S(x)})}{exp(\frac{\epsilon (c \cdot e^{\lambda})}{2 S(x)})}$ is at most $\frac{\epsilon}{2}$.

% Next, we proof the dilation property for $\lambda > 0$, The ratio of $\frac{h(c)}{h(e^{\lambda}c)}$ is $\exp(\frac{\epsilon}{2} \cdot \frac{u(\hexpmech(x)) (1 - e^{\lambda})}{S(x)})$. Consider the event $G = \{ \hexpmech(x) : u(\hexpmech(x)) \leq \frac{S(x)}{(1 - e^{\lambda})}\}$. Under this event, the log-ratio above is at most $\frac{\epsilon}{2}$. The probability of $G$ under density $h(c)$ is $1 - \frac{\delta}{2}$. Thus, the probability of a given event $z$ is at most $Pr[c \cap G] + Pr[\overline{G}] \leq e^{\frac{\epsilon}{2}} Pr[e^{\lambda}c \cap G] + \frac{\delta}{2} \leq e^{\frac{\epsilon}{2}} Pr[e^{\lambda}c] + \frac{\delta}{2}$.\\


% \textbf{Detail proof:}
% \begin{itemize}

% 	\item $\lambda < 0$

% 		The left hand side will always be smaller than 0 and the right hand side greater than 0. This will always holds, i.e.
% 		\begin{equation*}
% 		\end{equation*}
% 	\item $\lambda > 0$


% Because $\hat{s} = u(r)$ where $r \thicksim \hexpmech(x)$, we can substitute $\hat{s}$ with $u(\hexpmech(x))$. Then, what we need to proof under the case $\lambda > 0$ is:
% \begin{equation*}
% u(\hexpmech(x)) \leq \frac{S(x)}{(1 - e ^ {\lambda})}
% \end{equation*}
% By applying the accuracy property of exponential mechanism, we bound the probability that the equation holds with probability:
% \begin{equation*}
% \begin{split}
% Pr[u(\hexpmech(x)) \leq \frac{S(x)}{(1 - e ^ {\lambda})}] 
% & \leq \frac{|\mathcal{R}|exp(\frac{\epsilon S(x)}{(1 - e ^ {\lambda})}/2 S(x))}{|\mathcal{R}_{OPT}| exp(\epsilon OPT_{u(x)}/2 S(x))}\\
% \end{split}
% \end{equation*}

% In our Bayesian Inference mechanism, the size of the candidate set $\mathcal{R}$ is equal to the size of observed data set plus 1, i.e., $n + 1$, and $OPT_{u(x)} = 0$, then we have:
% \begin{equation*}
% \begin{split}
% Pr[u(\hexpmech(x)) \leq \frac{S(x)}{(1 - e ^ {\lambda})}] 
% & = (n + 1)exp(\frac{\epsilon S(x)}{(1 - e ^ {\lambda})}/2 S(x))\\
% & = (n + 1)exp(\frac{\epsilon}{2 (1 - e ^ {\lambda})})\\
% \end{split}
% \end{equation*}

% When we set $\lambda \leq \ln(1 - \frac{\epsilon}{2 \ln (\frac{\delta}{2 (n + 1)})})$, it is easily to derive that $Pr[u(\hexpmech(x)) \leq \frac{S(x)}{(1 - e ^ {\lambda})}] \leq \frac{\delta}{2}$.

% \end{itemize}

% \end{proof}

% \subsection{Privacy Analysis}
% \begin{lem}
% \label{lem_hexpmech_privacy}
% $\hexpmech$ is $(\epsilon, \delta)$-differential privacy.
% \end{lem}

% \begin{proof}
% of Lemma \ref{lem_hexpmech_privacy}: For all neighboring $x, y \in D^n$ and all sets $\mathcal{S}$, we need to show that:
% \begin{equation*}
% \underset{z \thicksim \hexpmech(x)}{Pr}[ z \in \mathcal{S}] \leq e^{\epsilon} \underset{z \thicksim \hexpmech(y)}{Pr}[z \in \mathcal{S}] + \delta. 
% \end{equation*}
% Given that $2\Big( \underset{z \thicksim \hexpmech(x)}{Pr}[ z \in \mathcal{S}]\Big) = \underset{z \thicksim \hexpmech(x)}{Pr}[ u(x,z) \in \mathcal{U}]$, let $\mathcal{U}_1 = \frac{u(y,z) - u(x,z)}{S(x)}$, $\mathcal{U}_2 = \mathcal{U} + \mathcal{U}_1$ and $\mathcal{U}_3 = \mathcal{U}_2 \cdot \frac{S(x)}{S(y)} \cdot \ln(\frac{NL(x)}{NL(y)})$. Then,

% \begin{equation*}
% \begin{split}
% 2\Big( \underset{z \thicksim \hexpmech(x)}{Pr}[ z \in \mathcal{S}]\Big)
% & = \underset{z \thicksim \hexpmech(x)}{Pr}[ u(x,z) \in \mathcal{U}]\\
% & \leq e^{\epsilon / 2} \cdot \underset{z \thicksim \hexpmech(x)}{Pr}[ u(x,z) \in \mathcal{U}_2]\\
% & \leq e^{\epsilon} \cdot \underset{z \thicksim \hexpmech(x)}{Pr}[ u(x,z) \in \mathcal{U}_3] + e^{\epsilon/2} \cdot \frac{\delta'}{2}\\
% & = e^{\epsilon} \cdot \underset{z \thicksim \hexpmech(y)}{Pr}[ u(y,z) \in \mathcal{U}] + \delta = 2\Big( e^{\epsilon} \cdot \underset{z \thicksim \hexpmech(x)}{Pr}[ z \in \mathcal{S}] \Big) + \delta\\
% \end{split}
% \end{equation*}

% The first inequality holds by the sliding property, since the $\mathcal{U}_1 \geq -S(x)$. The second inequality holds by the dilation property, since $\frac{S(x)}{S(y)} \cdot \ln(\frac{NL(x)}{NL(y)}) \leq 1 - \frac{\epsilon}{2 \ln (\frac{\delta}{2 (n + 1)})}$.

% \end{proof}

\section{Preliminary Experimental Results}
\label{sec_experiment}


\subsection{Accuracy Trade-off Evaluation wrt. Different Variables}
\label{subsec_vs_variables}

In this section, we evaluate the accuracy wrt. four variables, including data size, dimension, data variance and prior distribution, and some combinations of these variables. We experiment 1000 times under each value of variables and produce 4-quantile plots for each variable. In following 4-quantile plots, the y-axis is accuracy measured by Hellinger distance, x-axis is different value of variables. The blue boxes in plots represent our exponential mechanism and the next yellow box represents the Laplace mechanism under the same setting.

\subsubsection{Accuracy Evaluation wrt. Datasize}
\label{subsubsec_vs_datasize}

\begin{figure}[ht]
\begin{center}
\centering
  \subfigure[two dimensions with $\betad (1,1)$ prior distribution]{
    \includegraphics[width=0.3\textwidth]{accuracy_vs_datasize_1_1.eps}}
  \subfigure[three dimensions with $\dirichlet (1,1,1)$ prior distribution]{
    \includegraphics[width=0.3\textwidth]{accuracy_vs_datasize_1_1_1.eps}} 
\caption{Accuracy measurement based on Hellinger distance wrt. different datasizes. Settings: observed data are uniformly distributed, $\epsilon = 0.8$ and $\delta = 0.00000001$}
\label{fig_vs_datasize}
\end{center}
\end{figure}

In Fig. \ref{fig_vs_datasize}, both of the two plots show that when data size go larger, accuracy of our exponential mechanism are decreasing. In Fig. \ref{fig_vs_datasize}(a), when the data size is smaller than 12, we can beta Laplace mechanism but fail when data size larger than or equal to 12. Same as in Fig. \ref{fig_vs_datasize}(b), we can beat Laplace mechanism when data size is smaller than 15 and fail otherwise.

\subsubsection{Accuracy Evaluation wrt. Dimensions}
\label{subsubsec_vs_dimension}


\begin{figure}[ht]
\centering
\includegraphics[width=0.45\textwidth]{accuracy_vs_dimension.eps}
\caption{Accuracy measurement based on Hellinger distance wrt. different dimensions and data size. Settings: observed data are uniformly distributed, $\epsilon = 0.8$ and $\delta = 0.00000001$, prior distributions are all $1$ in every dimension}
\label{fig_vs_dimension}
\end{figure}

In Fig. \ref{fig_vs_dimension}, x-axis are observed data sets of different size and dimensions. The plot shows that dimensions have similar influence on our exponential mechanism and the Laplace mechanism. Accuracy of two mechanisms both decrease when dimensions go larger. We will be beat by Laplace mechanism when data size increase but will not be affected when dimensions increase. In other words, dimension has little influence on whether we will beat Laplace mechanism.


\subsubsection{Accuracy Evaluation wrt. Data variance}
\label{subsubsec_vs_variance}

\begin{figure}[ht]
\centering
\includegraphics[width=0.45\textwidth]{accuracy_vs_mean_1_1.eps}
\caption{Accuracy measurement based on Hellinger distance wrt. different data variance. Settings: $\epsilon = 0.8$ and $\delta = 0.00000001$, prior distributions are all $1$ in every dimension}
\label{fig_vs_variance}
\end{figure}

In Fig. \ref{fig_vs_variance}, x-axis are observed data sets of different variances (or means). We study this variable under two-dimension $\betad$ distribution in order to be concise. It shows that our mechanism's accuracy is better when data variance go smaller, meanwhile Laplace mechanism go worse. We will beat Laplace mechanism when observed data are more uniformly.



\subsubsection{Accuracy Evaluation wrt. Prior Distribution}
\label{subsubsec_vs_prior}

\begin{figure}[ht]
\centering
\includegraphics[width=0.45\textwidth]{accuracy_vs_prior_5_5_5.eps}
\caption{Accuracy measurement based on Hellinger distance wrt. different prior distribution. Settings: $\epsilon = 0.8$ and $\delta = 0.00000001$, observed data set is: $[5,5,5]$}
\label{fig_vs_prior}
\end{figure}

In Fig. \ref{fig_vs_prior}, we study this variable under setting that observed data set is $[5,5,5]$ because in Fig. \ref{fig_vs_datasize} Laplace mechanism beat us when data size is 15 and uniformly distributed. The plot shows that in the beginning we cannot beat Laplace but when prior distribution grow larger, we perform better and better and beat Laplace mechanism finally.



% \subsubsection{Accuracy Evaluation wrt. Prior Distribution and Data Variance}
% \label{subsubsec_vs_prior_variance}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.45\textwidth]{Accuracy_VS_Prior_mean.eps}
% \caption{Accuracy measurement based on Hellinger distance wrt. different prior distribution and data variances. Settings: $\epsilon = 0.8$ and $\delta = 0.00000001$, observed data sets are $[10,10,10]$ and $[1,1,28]$ and prior distributions are range from $[20,20,20]$ to $[76,76,76]$}
% \label{fig_vs_prior_variance}
% \end{figure}

% Here, we change the prior distribution and data variance in the same time. As shown in Fig. \ref{fig_vs_prior_variance}, our exponential mechanism do better in uniform data set than in edging data set while Laplace mechanism on the contrary. Moreover, our mechanism is improving continuously and significantly as prior distribution increasing while Laplace mechanism isn't.


\section{Conclusions}
We can obtain some preliminary conclusions: 
\begin{enumerate}
	\item We can beat Laplace mechanism when data size is small.
	\item We will beat Laplace mechanism when observed data are more uniformly.
  \item When prior distribution grow larger, we perform better and better and beat Laplace mechanism finally.
\end{enumerate} 

In consequence, we have a better accuracy in small data size, in larger prior and in uniformly data.



\bibliographystyle{ACM-Reference-Format}
\bibliography{bayesian.bib}

\end{document}
